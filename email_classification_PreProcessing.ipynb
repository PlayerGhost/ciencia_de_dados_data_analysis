{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import validators\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CARREGANDO BASE DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"Database/enron_spam_data.csv\")\n",
    "database = database.drop(columns=[\"Message ID\",\"Date\"])\n",
    "database = database.replace({\"ham\":0,\"spam\":1})\n",
    "database = database.drop(columns=[\"Subject\"])\n",
    "database = database.dropna()\n",
    "database = database.sample(frac=1)\n",
    "target = database[\"Spam/Ham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    ponctuation = list(punctuation)\n",
    "\n",
    "    for i in ponctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "stopWords = set(stopwords.words('english')  + list(punctuation) + list(STOPWORDS))\n",
    "stopWords.add(\"subject\")\n",
    "stem = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "regex = re.compile(r'[\\w\\.-]+@[\\w\\.-]+(\\.[\\w]+)+')\n",
    "\n",
    "def wordsPreProcessing(email):\n",
    "    if email is None:\n",
    "        return 'empty'\n",
    "\n",
    "    newText = \"\"\n",
    "\n",
    "    e = email.split()\n",
    "\n",
    "    for i in range(0,len(e)):\n",
    "        if validators.url(e[i]):\n",
    "            e[i] = \"URLLL\"\n",
    "\n",
    "        if re.fullmatch(regex,e[i]):\n",
    "            e[i] = \"EMAILLL\"\n",
    "\n",
    "    for text in word_tokenize(email.lower()):\n",
    "        text = removePunctuation(text)\n",
    "        if text not in stopWords and not text.isdigit():\n",
    "            newText += lemmatizer.lemmatize(text) + \" \"\n",
    "\n",
    "    return newText\n",
    "\n",
    "def NERProcessing(emails):\n",
    "    emailsPreProcessedLocal = []\n",
    "\n",
    "    spacy.require_gpu()\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "    NER.disable_pipe(\"parser\")\n",
    "    NER.add_pipe(\"sentencizer\")\n",
    "    NER.add_pipe(\"merge_entities\")\n",
    "\n",
    "    for doc in NER.pipe(emails, n_process=2):\n",
    "        emailsPreProcessedAux = []\n",
    "\n",
    "        for t in doc:\n",
    "            if not t.ent_type_:\n",
    "                emailsPreProcessedAux.append(t.text)\n",
    "            else:\n",
    "                emailsPreProcessedAux.append((t.ent_type_ + t.ent_type_[-1]).upper())\n",
    "\n",
    "        emailsPreProcessedAux = \" \".join(emailsPreProcessedAux)\n",
    "        emailsPreProcessedLocal.append(wordsPreProcessing(emailsPreProcessedAux))\n",
    "\n",
    "    return emailsPreProcessedLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "textos = []\n",
    "ini = datetime.now()\n",
    "\n",
    "emailsPreProcessed = NERProcessing(database[\"Message\"])\n",
    "\n",
    "print(datetime.now() - ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NERDataframe = pd.DataFrame(emailsPreProcessed, columns=[\"email\"])\n",
    "NERDataframe[\"target\"] = target.values\n",
    "\n",
    "NERDataframe.to_csv('Database/dataBaseWithNER.csv')\n",
    "NERDataframe"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbc4fa9a3b3b5fde6e72743f268de4fdaf589af2bf8366208cd2cc04db97402e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('conda_data_science_gpu_on_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
