{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Carregando base de dados  pré-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start date     hourahead timee  cardinall  hou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service long desk  price structure deal quote ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start date  cardinall    hourahead timee  card...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start date     hourahead timee  cardinall  anc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardinall deliverable revenue management marke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33340</th>\n",
       "      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33341</th>\n",
       "      <td>cardinall step away hot naked webcam girl liv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33342</th>\n",
       "      <td>need pill increase performance click  seroius ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33343</th>\n",
       "      <td>datee final nom       inlet hpl  eastrans  car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33344</th>\n",
       "      <td>ordinall time  offering male enhancement perfo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   email  target\n",
       "0      start date     hourahead timee  cardinall  hou...       0\n",
       "1      service long desk  price structure deal quote ...       0\n",
       "2      start date  cardinall    hourahead timee  card...       0\n",
       "3      start date     hourahead timee  cardinall  anc...       0\n",
       "4      cardinall deliverable revenue management marke...       0\n",
       "...                                                  ...     ...\n",
       "33340  bio  matrix scientific group   symbo   bmxg  p...       1\n",
       "33341   cardinall step away hot naked webcam girl liv...       1\n",
       "33342  need pill increase performance click  seroius ...       1\n",
       "33343  datee final nom       inlet hpl  eastrans  car...       0\n",
       "33344  ordinall time  offering male enhancement perfo...       1\n",
       "\n",
       "[33341 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "targets = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Representação vetorial GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.0.attn.c_proj.bias', 'h.9.ln_2.weight', 'h.8.attn.c_proj.bias', 'h.8.ln_2.bias', 'h.0.mlp.c_fc.bias', 'h.7.attn.bias', 'h.1.ln_1.bias', 'h.10.attn.c_attn.bias', 'h.3.attn.c_attn.weight', 'h.6.mlp.c_proj.weight', 'h.8.attn.c_attn.weight', 'h.5.attn.c_attn.weight', 'h.2.attn.c_proj.bias', 'h.9.attn.c_proj.weight', 'h.8.ln_1.bias', 'h.2.ln_1.weight', 'h.3.mlp.c_fc.bias', 'h.2.ln_1.bias', 'h.3.attn.c_proj.bias', 'h.4.attn.c_proj.bias', 'h.4.ln_2.bias', 'h.0.attn.c_attn.bias', 'h.0.ln_2.bias', 'h.10.attn.c_attn.weight', 'h.4.attn.c_proj.weight', 'h.5.attn.bias', 'h.11.mlp.c_fc.weight', 'h.3.ln_1.weight', 'h.5.ln_1.bias', 'h.4.mlp.c_fc.bias', 'h.11.ln_1.weight', 'h.7.ln_2.weight', 'h.11.mlp.c_proj.weight', 'h.2.attn.c_attn.bias', 'h.7.mlp.c_proj.weight', 'h.7.attn.c_attn.weight', 'h.1.attn.c_proj.weight', 'h.2.ln_2.bias', 'h.1.ln_2.weight', 'h.10.mlp.c_fc.weight', 'h.5.mlp.c_fc.weight', 'h.10.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.8.ln_1.weight', 'h.6.mlp.c_fc.bias', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.6.mlp.c_fc.weight', 'h.8.attn.bias', 'h.11.ln_2.bias', 'h.3.ln_1.bias', 'h.3.ln_2.bias', 'h.11.attn.c_attn.weight', 'h.11.attn.c_proj.weight', 'h.0.mlp.c_proj.weight', 'h.7.mlp.c_proj.bias', 'h.8.mlp.c_proj.bias', 'h.7.mlp.c_fc.weight', 'h.5.ln_2.weight', 'h.7.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.2.ln_2.weight', 'h.11.ln_2.weight', 'h.11.mlp.c_proj.bias', 'h.1.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.3.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.6.ln_2.bias', 'h.1.attn.c_proj.bias', 'h.0.mlp.c_fc.weight', 'h.2.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.5.ln_2.bias', 'h.6.ln_1.weight', 'h.4.attn.c_attn.weight', 'h.10.ln_1.bias', 'h.10.mlp.c_proj.weight', 'h.5.mlp.c_proj.weight', 'h.6.attn.c_attn.weight', 'ln_f.weight', 'h.3.attn.bias', 'h.6.mlp.c_proj.bias', 'h.9.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.6.attn.bias', 'h.10.attn.bias', 'h.2.attn.c_attn.weight', 'h.3.attn.c_proj.weight', 'h.7.mlp.c_fc.bias', 'h.2.attn.c_proj.weight', 'h.2.mlp.c_fc.bias', 'h.8.mlp.c_fc.bias', 'h.5.mlp.c_proj.bias', 'h.2.attn.bias', 'h.9.attn.c_attn.bias', 'h.4.attn.c_attn.bias', 'h.11.attn.c_attn.bias', 'h.5.attn.c_proj.bias', 'h.1.ln_1.weight', 'h.8.mlp.c_proj.weight', 'h.8.attn.c_attn.bias', 'wpe.weight', 'h.9.ln_2.bias', 'h.4.ln_2.weight', 'h.6.attn.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.8.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.10.ln_2.bias', 'h.0.attn.c_attn.weight', 'wte.weight', 'h.9.ln_1.weight', 'h.3.mlp.c_fc.weight', 'h.1.ln_2.bias', 'h.10.ln_1.weight', 'h.11.ln_1.bias', 'h.3.attn.c_attn.bias', 'h.4.mlp.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.11.attn.bias', 'h.0.ln_1.weight', 'h.1.mlp.c_fc.weight', 'h.4.mlp.c_fc.weight', 'h.2.mlp.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.1.attn.bias', 'h.6.ln_1.bias', 'h.9.attn.bias', 'ln_f.bias', 'h.4.attn.bias', 'h.9.attn.c_attn.weight', 'h.11.attn.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.10.attn.c_proj.bias', 'h.6.ln_2.weight', 'h.6.attn.c_proj.weight', 'h.7.ln_2.bias', 'h.10.mlp.c_proj.bias', 'h.2.mlp.c_fc.weight', 'h.7.ln_1.bias', 'h.5.mlp.c_fc.bias', 'h.0.ln_2.weight', 'h.7.attn.c_attn.bias', 'h.9.mlp.c_proj.weight', 'h.4.ln_1.weight', 'h.5.attn.c_proj.weight', 'h.9.mlp.c_proj.bias', 'h.4.ln_1.bias', 'h.5.ln_1.weight', 'h.11.mlp.c_fc.bias', 'h.3.mlp.c_proj.weight', 'h.0.attn.bias', 'h.10.mlp.c_fc.bias', 'h.10.ln_2.weight', 'h.8.attn.c_proj.weight', 'h.8.ln_2.weight']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.8.attn.masked_bias', 'gpt2.h.7.ln_2.bias', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.4.ln_2.bias', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.10.attn.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.10.ln_1.weight', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.5.attn.bias', 'gpt2.h.5.ln_2.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.8.attn.bias', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.2.ln_1.bias', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.0.ln_1.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.7.attn.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.6.ln_1.weight', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.5.ln_2.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.0.ln_1.bias', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.1.ln_2.bias', 'gpt2.h.8.ln_1.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.9.ln_2.bias', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.6.ln_1.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.1.ln_1.bias', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.4.ln_1.weight', 'gpt2.ln_f.bias', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.4.attn.bias', 'gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.9.ln_1.weight', 'gpt2.h.6.ln_2.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.1.ln_1.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.attn.c_proj.weight', 'gpt2.h.11.ln_1.bias', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.7.ln_1.bias', 'gpt2.wpe.weight', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.11.attn.bias', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.h.6.ln_2.bias', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.7.ln_1.weight', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.2.attn.bias', 'gpt2.h.2.ln_2.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.5.ln_1.weight', 'gpt2.h.2.ln_1.weight', 'gpt2.h.11.ln_1.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.8.ln_1.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.ln_f.weight', 'gpt2.h.1.ln_2.weight', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.6.attn.bias', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.9.attn.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.3.attn.bias', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.7.ln_2.weight', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.3.ln_2.bias', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.10.ln_2.bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.wte.weight', 'gpt2.h.0.attn.bias', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.4.ln_1.bias', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.ln_2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33341, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RepresentationModel(\n",
    "    model_type=\"gpt2\",\n",
    "    model_name=\"gpt2\",\n",
    "    use_cuda=True,\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "vectorialRepresentation = model.encode_sentences(emailsText, combine_strategy=\"mean\")\n",
    "vectorialRepresentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.578063</td>\n",
       "      <td>-1.562219</td>\n",
       "      <td>0.733528</td>\n",
       "      <td>-0.058638</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>1.620385</td>\n",
       "      <td>1.365647</td>\n",
       "      <td>-0.572078</td>\n",
       "      <td>-0.815387</td>\n",
       "      <td>-2.781020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946592</td>\n",
       "      <td>1.232427</td>\n",
       "      <td>0.894165</td>\n",
       "      <td>0.027148</td>\n",
       "      <td>0.232668</td>\n",
       "      <td>0.037412</td>\n",
       "      <td>-0.670586</td>\n",
       "      <td>-0.364401</td>\n",
       "      <td>-0.319977</td>\n",
       "      <td>-1.053342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.622657</td>\n",
       "      <td>0.053144</td>\n",
       "      <td>1.068417</td>\n",
       "      <td>-0.368139</td>\n",
       "      <td>-1.178293</td>\n",
       "      <td>1.251067</td>\n",
       "      <td>0.150130</td>\n",
       "      <td>0.676408</td>\n",
       "      <td>-0.146761</td>\n",
       "      <td>-1.484483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231804</td>\n",
       "      <td>1.282375</td>\n",
       "      <td>1.661981</td>\n",
       "      <td>-0.441529</td>\n",
       "      <td>-0.768833</td>\n",
       "      <td>-0.983972</td>\n",
       "      <td>-1.993457</td>\n",
       "      <td>0.640664</td>\n",
       "      <td>-0.010069</td>\n",
       "      <td>-1.850842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.672441</td>\n",
       "      <td>-1.871471</td>\n",
       "      <td>1.020190</td>\n",
       "      <td>0.274535</td>\n",
       "      <td>-0.788345</td>\n",
       "      <td>1.747756</td>\n",
       "      <td>1.122822</td>\n",
       "      <td>0.102061</td>\n",
       "      <td>-0.308725</td>\n",
       "      <td>-2.670972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768910</td>\n",
       "      <td>1.679851</td>\n",
       "      <td>0.357598</td>\n",
       "      <td>0.624521</td>\n",
       "      <td>-0.179420</td>\n",
       "      <td>0.110053</td>\n",
       "      <td>-0.643694</td>\n",
       "      <td>-0.172248</td>\n",
       "      <td>-0.226315</td>\n",
       "      <td>-1.465842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.597837</td>\n",
       "      <td>-1.746223</td>\n",
       "      <td>1.017650</td>\n",
       "      <td>0.173475</td>\n",
       "      <td>-0.535336</td>\n",
       "      <td>1.816171</td>\n",
       "      <td>1.490823</td>\n",
       "      <td>-0.008094</td>\n",
       "      <td>-0.469706</td>\n",
       "      <td>-2.843270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887539</td>\n",
       "      <td>1.504657</td>\n",
       "      <td>0.935981</td>\n",
       "      <td>0.599522</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>-0.047194</td>\n",
       "      <td>-0.673326</td>\n",
       "      <td>-0.182649</td>\n",
       "      <td>-0.176225</td>\n",
       "      <td>-1.107678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.028337</td>\n",
       "      <td>-0.182032</td>\n",
       "      <td>1.087843</td>\n",
       "      <td>0.138309</td>\n",
       "      <td>-0.664889</td>\n",
       "      <td>0.816702</td>\n",
       "      <td>0.862890</td>\n",
       "      <td>0.121081</td>\n",
       "      <td>-0.749520</td>\n",
       "      <td>-1.705546</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294798</td>\n",
       "      <td>1.596552</td>\n",
       "      <td>0.873833</td>\n",
       "      <td>0.890408</td>\n",
       "      <td>-0.913309</td>\n",
       "      <td>-0.340488</td>\n",
       "      <td>-1.247871</td>\n",
       "      <td>-0.306625</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>-1.804885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33336</th>\n",
       "      <td>0.764639</td>\n",
       "      <td>-1.408462</td>\n",
       "      <td>1.829600</td>\n",
       "      <td>-0.525103</td>\n",
       "      <td>-1.596769</td>\n",
       "      <td>2.506067</td>\n",
       "      <td>0.974421</td>\n",
       "      <td>-0.115114</td>\n",
       "      <td>0.072327</td>\n",
       "      <td>-1.600080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551293</td>\n",
       "      <td>2.203605</td>\n",
       "      <td>0.901629</td>\n",
       "      <td>0.518770</td>\n",
       "      <td>0.565980</td>\n",
       "      <td>-0.740531</td>\n",
       "      <td>-0.685751</td>\n",
       "      <td>0.523401</td>\n",
       "      <td>-0.779780</td>\n",
       "      <td>-1.579606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33337</th>\n",
       "      <td>-0.216924</td>\n",
       "      <td>-1.351918</td>\n",
       "      <td>1.099144</td>\n",
       "      <td>0.204871</td>\n",
       "      <td>-1.159167</td>\n",
       "      <td>1.484302</td>\n",
       "      <td>0.503977</td>\n",
       "      <td>0.199203</td>\n",
       "      <td>0.008998</td>\n",
       "      <td>-2.148698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159228</td>\n",
       "      <td>1.983267</td>\n",
       "      <td>0.423999</td>\n",
       "      <td>0.937024</td>\n",
       "      <td>-0.492670</td>\n",
       "      <td>-0.496444</td>\n",
       "      <td>-0.549892</td>\n",
       "      <td>-0.349489</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>-2.001184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33338</th>\n",
       "      <td>0.546038</td>\n",
       "      <td>-2.470647</td>\n",
       "      <td>1.207605</td>\n",
       "      <td>1.797069</td>\n",
       "      <td>-1.467915</td>\n",
       "      <td>1.701314</td>\n",
       "      <td>1.534451</td>\n",
       "      <td>-0.706129</td>\n",
       "      <td>-0.954505</td>\n",
       "      <td>-2.490199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.406200</td>\n",
       "      <td>1.865775</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>0.946446</td>\n",
       "      <td>-0.232542</td>\n",
       "      <td>0.050942</td>\n",
       "      <td>-0.758569</td>\n",
       "      <td>-0.091412</td>\n",
       "      <td>0.397039</td>\n",
       "      <td>-0.996819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33339</th>\n",
       "      <td>0.305662</td>\n",
       "      <td>-1.203844</td>\n",
       "      <td>0.980389</td>\n",
       "      <td>0.851744</td>\n",
       "      <td>-1.239764</td>\n",
       "      <td>2.163097</td>\n",
       "      <td>0.364815</td>\n",
       "      <td>-0.100628</td>\n",
       "      <td>0.076471</td>\n",
       "      <td>-2.574973</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302633</td>\n",
       "      <td>1.986321</td>\n",
       "      <td>1.218455</td>\n",
       "      <td>0.840711</td>\n",
       "      <td>0.548881</td>\n",
       "      <td>0.348737</td>\n",
       "      <td>-1.149769</td>\n",
       "      <td>0.356195</td>\n",
       "      <td>-0.494669</td>\n",
       "      <td>-0.772472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33340</th>\n",
       "      <td>-0.226237</td>\n",
       "      <td>-1.578368</td>\n",
       "      <td>1.017262</td>\n",
       "      <td>0.603601</td>\n",
       "      <td>-1.769842</td>\n",
       "      <td>1.046477</td>\n",
       "      <td>0.912931</td>\n",
       "      <td>-0.714981</td>\n",
       "      <td>-0.611790</td>\n",
       "      <td>-0.613617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>1.667148</td>\n",
       "      <td>1.252112</td>\n",
       "      <td>0.324927</td>\n",
       "      <td>-0.585878</td>\n",
       "      <td>-0.798224</td>\n",
       "      <td>-0.734891</td>\n",
       "      <td>0.219252</td>\n",
       "      <td>0.509139</td>\n",
       "      <td>-1.124477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33341 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0      0.578063 -1.562219  0.733528 -0.058638  0.102807  1.620385  1.365647   \n",
       "1     -0.622657  0.053144  1.068417 -0.368139 -1.178293  1.251067  0.150130   \n",
       "2      0.672441 -1.871471  1.020190  0.274535 -0.788345  1.747756  1.122822   \n",
       "3      0.597837 -1.746223  1.017650  0.173475 -0.535336  1.816171  1.490823   \n",
       "4     -1.028337 -0.182032  1.087843  0.138309 -0.664889  0.816702  0.862890   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33336  0.764639 -1.408462  1.829600 -0.525103 -1.596769  2.506067  0.974421   \n",
       "33337 -0.216924 -1.351918  1.099144  0.204871 -1.159167  1.484302  0.503977   \n",
       "33338  0.546038 -2.470647  1.207605  1.797069 -1.467915  1.701314  1.534451   \n",
       "33339  0.305662 -1.203844  0.980389  0.851744 -1.239764  2.163097  0.364815   \n",
       "33340 -0.226237 -1.578368  1.017262  0.603601 -1.769842  1.046477  0.912931   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0     -0.572078 -0.815387 -2.781020  ...  0.946592  1.232427  0.894165   \n",
       "1      0.676408 -0.146761 -1.484483  ... -0.231804  1.282375  1.661981   \n",
       "2      0.102061 -0.308725 -2.670972  ...  0.768910  1.679851  0.357598   \n",
       "3     -0.008094 -0.469706 -2.843270  ...  0.887539  1.504657  0.935981   \n",
       "4      0.121081 -0.749520 -1.705546  ... -0.294798  1.596552  0.873833   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33336 -0.115114  0.072327 -1.600080  ...  0.551293  2.203605  0.901629   \n",
       "33337  0.199203  0.008998 -2.148698  ... -0.159228  1.983267  0.423999   \n",
       "33338 -0.706129 -0.954505 -2.490199  ... -0.406200  1.865775  1.540000   \n",
       "33339 -0.100628  0.076471 -2.574973  ... -0.302633  1.986321  1.218455   \n",
       "33340 -0.714981 -0.611790 -0.613617  ...  0.502650  1.667148  1.252112   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0      0.027148  0.232668  0.037412 -0.670586 -0.364401 -0.319977 -1.053342  \n",
       "1     -0.441529 -0.768833 -0.983972 -1.993457  0.640664 -0.010069 -1.850842  \n",
       "2      0.624521 -0.179420  0.110053 -0.643694 -0.172248 -0.226315 -1.465842  \n",
       "3      0.599522  0.054418 -0.047194 -0.673326 -0.182649 -0.176225 -1.107678  \n",
       "4      0.890408 -0.913309 -0.340488 -1.247871 -0.306625  0.000457 -1.804885  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33336  0.518770  0.565980 -0.740531 -0.685751  0.523401 -0.779780 -1.579606  \n",
       "33337  0.937024 -0.492670 -0.496444 -0.549892 -0.349489  0.037575 -2.001184  \n",
       "33338  0.946446 -0.232542  0.050942 -0.758569 -0.091412  0.397039 -0.996819  \n",
       "33339  0.840711  0.548881  0.348737 -1.149769  0.356195 -0.494669 -0.772472  \n",
       "33340  0.324927 -0.585878 -0.798224 -0.734891  0.219252  0.509139 -1.124477  \n",
       "\n",
       "[33341 rows x 768 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2Dataframe = pd.DataFrame(vectorialRepresentation)\n",
    "gpt2Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpt2Data = np.array(gpt2Dataframe)\n",
    "gpt2Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualização de dados com TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0          1  Target\n",
      "0     -14.011990 -71.692970       0\n",
      "1      15.829963  10.544511       0\n",
      "2      10.241787  52.096443       0\n",
      "3      69.784821  14.447907       0\n",
      "4      -6.443357  57.167374       0\n",
      "...          ...        ...     ...\n",
      "33336  52.482487   3.101025       1\n",
      "33337  -6.246800  37.836533       1\n",
      "33338  42.079762  -3.870044       1\n",
      "33339   5.700751  21.373949       0\n",
      "33340   2.333664  31.864038       1\n",
      "\n",
      "[33341 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# array_red = model.fit_transform(gpt2Dataframe)\n",
    "\n",
    "# df_tsne = pd.DataFrame(array_red)\n",
    "\n",
    "# df_tsne['Target'] = target\n",
    "# print(df_tsne)\n",
    "# df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "\n",
    "# df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "\n",
    "# plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "\n",
    "# plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gpt2Data = np.array(gpt2Data)\n",
    "\n",
    "gpt2Data = gpt2Data.reshape((gpt2Data.shape[0],1,gpt2Data.shape[1]))\n",
    "print(gpt2Data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 4s 3ms/step - loss: 0.3499 - accuracy: 0.8889 - val_loss: 0.1727 - val_accuracy: 0.9440 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9410 - val_loss: 0.1448 - val_accuracy: 0.9528 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9501 - val_loss: 0.1184 - val_accuracy: 0.9600 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1385 - accuracy: 0.9627 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1287 - accuracy: 0.9648 - val_loss: 0.1275 - val_accuracy: 0.9576 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0776 - accuracy: 0.9768 - val_loss: 0.0639 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0893 - accuracy: 0.9764 - val_loss: 0.0666 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.0559 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0609 - accuracy: 0.9839 - val_loss: 0.0539 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0718 - accuracy: 0.9827 - val_loss: 0.0682 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0445 - accuracy: 0.9865 - val_loss: 0.0527 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.0443 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9882 - val_loss: 0.0532 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9915 - val_loss: 0.0550 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0246 - accuracy: 0.9930 - val_loss: 0.0423 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9944 - val_loss: 0.0495 - val_accuracy: 0.9824 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0448 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0426 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0431 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.0464 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0476 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9961\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.0538 - val_accuracy: 0.9848 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0478 - val_accuracy: 0.9872 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0128 - accuracy: 0.9968 - val_loss: 0.0466 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0462 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.0456 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0448 - val_accuracy: 0.9880 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Score fold 1: loss de 0.09102164953947067; accuracy de 97.5287914276123%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3240 - accuracy: 0.8913 - val_loss: 0.1648 - val_accuracy: 0.9472 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1536 - accuracy: 0.9546 - val_loss: 0.1026 - val_accuracy: 0.9584 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1171 - accuracy: 0.9664 - val_loss: 0.0876 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9588 - val_loss: 0.2787 - val_accuracy: 0.8857 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.9499 - val_loss: 0.7121 - val_accuracy: 0.7066 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2149 - accuracy: 0.9278 - val_loss: 0.1107 - val_accuracy: 0.9560 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9687 - val_loss: 0.0864 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0813 - accuracy: 0.9749 - val_loss: 0.0697 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0574 - accuracy: 0.9812 - val_loss: 0.0616 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.0590 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0419 - accuracy: 0.9867 - val_loss: 0.0565 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.0586 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0774 - accuracy: 0.9845 - val_loss: 0.0612 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0520 - accuracy: 0.9876 - val_loss: 0.0577 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.0558 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0267 - accuracy: 0.9929 - val_loss: 0.0529 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0285 - accuracy: 0.9924 - val_loss: 0.0543 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0224 - accuracy: 0.9936 - val_loss: 0.0551 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0538 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 0.0588 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.0556 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0555 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "723/743 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9959\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0571 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0572 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0573 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0110 - accuracy: 0.9972 - val_loss: 0.0575 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "727/743 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0577 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Score fold 2: loss de 0.07085882872343063; accuracy de 97.69645929336548%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8760 - val_loss: 0.2134 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9490 - val_loss: 0.1158 - val_accuracy: 0.9544 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9676 - val_loss: 0.0948 - val_accuracy: 0.9648 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.9695 - val_loss: 0.0758 - val_accuracy: 0.9664 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0898 - accuracy: 0.9764 - val_loss: 0.0679 - val_accuracy: 0.9704 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.0603 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.9750 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1049 - accuracy: 0.9779 - val_loss: 0.2473 - val_accuracy: 0.8753 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1069 - accuracy: 0.9722 - val_loss: 0.0636 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0587 - accuracy: 0.9830 - val_loss: 0.0533 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0661 - accuracy: 0.9831 - val_loss: 0.0547 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0401 - accuracy: 0.9873 - val_loss: 0.0523 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0927 - accuracy: 0.9835 - val_loss: 0.0602 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0899 - accuracy: 0.9795 - val_loss: 0.0544 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9874 - val_loss: 0.0558 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0497 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 0.0532 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0515 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0463 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0253 - accuracy: 0.9931 - val_loss: 0.0453 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0457 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0203 - accuracy: 0.9947 - val_loss: 0.0481 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.0502 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 0.0512 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0589 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9922 - val_loss: 0.0557 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "  1/743 [..............................] - ETA: 1s - loss: 0.0097 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(gpt2Data, targets):\n",
    "    model = keras.models.Sequential([\n",
    "        #keras.layers.Conv1D(1000, kernel_size=1, activation=\"relu\",input_shape=gpt2Data.shape[1:]),\n",
    "        keras.layers.Conv1D(640, kernel_size=1, activation=\"relu\",input_shape=gpt2Data.shape[1:]),\n",
    "        keras.layers.MaxPooling1D(pool_size=1),\n",
    "        keras.layers.Flatten(),\n",
    "        #keras.layers.Dense(5000, activation=\"relu\"),\n",
    "        #keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(len(set(targets)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(gpt2Data[train], targets[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(gpt2Data[test], targets[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}