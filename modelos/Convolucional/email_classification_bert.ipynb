{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import  classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Carregando base de dados  pré-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start date     hourahead timee  cardinall  hou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>service long desk  price structure deal quote ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start date  cardinall    hourahead timee  card...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start date     hourahead timee  cardinall  anc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardinall deliverable revenue management marke...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33340</th>\n",
       "      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33341</th>\n",
       "      <td>cardinall step away hot naked webcam girl liv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33342</th>\n",
       "      <td>need pill increase performance click  seroius ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33343</th>\n",
       "      <td>datee final nom       inlet hpl  eastrans  car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33344</th>\n",
       "      <td>ordinall time  offering male enhancement perfo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33341 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   email  target\n",
       "0      start date     hourahead timee  cardinall  hou...       0\n",
       "1      service long desk  price structure deal quote ...       0\n",
       "2      start date  cardinall    hourahead timee  card...       0\n",
       "3      start date     hourahead timee  cardinall  anc...       0\n",
       "4      cardinall deliverable revenue management marke...       0\n",
       "...                                                  ...     ...\n",
       "33340  bio  matrix scientific group   symbo   bmxg  p...       1\n",
       "33341   cardinall step away hot naked webcam girl liv...       1\n",
       "33342  need pill increase performance click  seroius ...       1\n",
       "33343  datee final nom       inlet hpl  eastrans  car...       0\n",
       "33344  ordinall time  offering male enhancement perfo...       1\n",
       "\n",
       "[33341 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "targets = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Representação vetorial BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTextRepresentation: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33341, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RepresentationModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    use_cuda=True,\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "vectorialRepresentation = model.encode_sentences(emailsText, combine_strategy=\"mean\")\n",
    "vectorialRepresentation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.159428</td>\n",
       "      <td>-0.150673</td>\n",
       "      <td>0.320971</td>\n",
       "      <td>0.112358</td>\n",
       "      <td>0.459967</td>\n",
       "      <td>0.077280</td>\n",
       "      <td>0.211645</td>\n",
       "      <td>0.108739</td>\n",
       "      <td>-0.041492</td>\n",
       "      <td>-0.183632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251432</td>\n",
       "      <td>-0.163091</td>\n",
       "      <td>-0.116690</td>\n",
       "      <td>-0.070913</td>\n",
       "      <td>-0.048259</td>\n",
       "      <td>-0.037924</td>\n",
       "      <td>-0.119490</td>\n",
       "      <td>-0.189950</td>\n",
       "      <td>-0.203662</td>\n",
       "      <td>0.074954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071051</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>-0.089434</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.019528</td>\n",
       "      <td>0.109083</td>\n",
       "      <td>0.026950</td>\n",
       "      <td>0.204912</td>\n",
       "      <td>-0.180139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.284122</td>\n",
       "      <td>-0.540412</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>-0.095771</td>\n",
       "      <td>-0.203890</td>\n",
       "      <td>0.091056</td>\n",
       "      <td>-0.145161</td>\n",
       "      <td>-0.096876</td>\n",
       "      <td>-0.135908</td>\n",
       "      <td>0.060218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.151653</td>\n",
       "      <td>-0.164808</td>\n",
       "      <td>0.471539</td>\n",
       "      <td>-0.066901</td>\n",
       "      <td>0.267297</td>\n",
       "      <td>0.017360</td>\n",
       "      <td>-0.074956</td>\n",
       "      <td>0.166499</td>\n",
       "      <td>-0.042147</td>\n",
       "      <td>-0.080027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109118</td>\n",
       "      <td>-0.118454</td>\n",
       "      <td>-0.086577</td>\n",
       "      <td>-0.057226</td>\n",
       "      <td>0.129574</td>\n",
       "      <td>-0.021993</td>\n",
       "      <td>-0.200361</td>\n",
       "      <td>-0.175351</td>\n",
       "      <td>-0.030066</td>\n",
       "      <td>0.166013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091722</td>\n",
       "      <td>-0.252287</td>\n",
       "      <td>0.291835</td>\n",
       "      <td>0.154237</td>\n",
       "      <td>0.285827</td>\n",
       "      <td>-0.033622</td>\n",
       "      <td>-0.024027</td>\n",
       "      <td>0.107403</td>\n",
       "      <td>-0.000545</td>\n",
       "      <td>-0.192616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225206</td>\n",
       "      <td>-0.128331</td>\n",
       "      <td>-0.235366</td>\n",
       "      <td>-0.151748</td>\n",
       "      <td>-0.025532</td>\n",
       "      <td>0.023671</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>-0.086625</td>\n",
       "      <td>-0.004205</td>\n",
       "      <td>0.172869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028974</td>\n",
       "      <td>-0.141580</td>\n",
       "      <td>0.607358</td>\n",
       "      <td>0.108809</td>\n",
       "      <td>0.400952</td>\n",
       "      <td>-0.087711</td>\n",
       "      <td>-0.108502</td>\n",
       "      <td>0.262436</td>\n",
       "      <td>-0.031409</td>\n",
       "      <td>-0.208465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080698</td>\n",
       "      <td>-0.173298</td>\n",
       "      <td>0.043021</td>\n",
       "      <td>-0.124838</td>\n",
       "      <td>-0.122861</td>\n",
       "      <td>-0.152739</td>\n",
       "      <td>-0.130289</td>\n",
       "      <td>-0.272174</td>\n",
       "      <td>-0.106081</td>\n",
       "      <td>-0.142323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33336</th>\n",
       "      <td>-0.201023</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>-0.040250</td>\n",
       "      <td>0.472595</td>\n",
       "      <td>0.059005</td>\n",
       "      <td>-0.223889</td>\n",
       "      <td>0.168916</td>\n",
       "      <td>-0.019692</td>\n",
       "      <td>-0.068533</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283789</td>\n",
       "      <td>0.086214</td>\n",
       "      <td>0.281354</td>\n",
       "      <td>0.030180</td>\n",
       "      <td>0.144887</td>\n",
       "      <td>-0.240283</td>\n",
       "      <td>-0.269940</td>\n",
       "      <td>-0.195859</td>\n",
       "      <td>-0.090531</td>\n",
       "      <td>-0.038818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33337</th>\n",
       "      <td>0.045905</td>\n",
       "      <td>-0.145504</td>\n",
       "      <td>0.600411</td>\n",
       "      <td>0.147447</td>\n",
       "      <td>0.138597</td>\n",
       "      <td>-0.070919</td>\n",
       "      <td>0.243944</td>\n",
       "      <td>0.133416</td>\n",
       "      <td>-0.177918</td>\n",
       "      <td>-0.203445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>-0.050800</td>\n",
       "      <td>0.085909</td>\n",
       "      <td>-0.260432</td>\n",
       "      <td>-0.009710</td>\n",
       "      <td>0.090369</td>\n",
       "      <td>-0.170151</td>\n",
       "      <td>-0.095905</td>\n",
       "      <td>-0.061700</td>\n",
       "      <td>-0.136271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33338</th>\n",
       "      <td>-0.052234</td>\n",
       "      <td>-0.027882</td>\n",
       "      <td>0.484133</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.134670</td>\n",
       "      <td>0.035107</td>\n",
       "      <td>0.068209</td>\n",
       "      <td>0.120231</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>-0.294419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157346</td>\n",
       "      <td>-0.069377</td>\n",
       "      <td>0.014211</td>\n",
       "      <td>-0.028526</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>-0.002726</td>\n",
       "      <td>-0.118924</td>\n",
       "      <td>0.022453</td>\n",
       "      <td>0.061370</td>\n",
       "      <td>0.090219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33339</th>\n",
       "      <td>-0.226833</td>\n",
       "      <td>-0.098093</td>\n",
       "      <td>0.445973</td>\n",
       "      <td>-0.028613</td>\n",
       "      <td>0.256372</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>-0.050134</td>\n",
       "      <td>0.159709</td>\n",
       "      <td>0.134332</td>\n",
       "      <td>-0.035709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012439</td>\n",
       "      <td>-0.048856</td>\n",
       "      <td>0.105855</td>\n",
       "      <td>-0.005054</td>\n",
       "      <td>0.283087</td>\n",
       "      <td>-0.077617</td>\n",
       "      <td>-0.224488</td>\n",
       "      <td>-0.029133</td>\n",
       "      <td>-0.032732</td>\n",
       "      <td>-0.195680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33340</th>\n",
       "      <td>-0.131739</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.562934</td>\n",
       "      <td>0.016376</td>\n",
       "      <td>0.139309</td>\n",
       "      <td>-0.102250</td>\n",
       "      <td>0.267339</td>\n",
       "      <td>0.035725</td>\n",
       "      <td>0.026772</td>\n",
       "      <td>-0.307518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101262</td>\n",
       "      <td>-0.228757</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>-0.072525</td>\n",
       "      <td>0.044891</td>\n",
       "      <td>-0.259199</td>\n",
       "      <td>-0.250731</td>\n",
       "      <td>-0.136858</td>\n",
       "      <td>-0.125877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33341 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.159428 -0.150673  0.320971  0.112358  0.459967  0.077280  0.211645   \n",
       "1      0.071051  0.006274  0.291100 -0.089434  0.006165  0.019528  0.109083   \n",
       "2     -0.151653 -0.164808  0.471539 -0.066901  0.267297  0.017360 -0.074956   \n",
       "3     -0.091722 -0.252287  0.291835  0.154237  0.285827 -0.033622 -0.024027   \n",
       "4     -0.028974 -0.141580  0.607358  0.108809  0.400952 -0.087711 -0.108502   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33336 -0.201023 -0.001996  0.590600 -0.040250  0.472595  0.059005 -0.223889   \n",
       "33337  0.045905 -0.145504  0.600411  0.147447  0.138597 -0.070919  0.243944   \n",
       "33338 -0.052234 -0.027882  0.484133  0.026257  0.134670  0.035107  0.068209   \n",
       "33339 -0.226833 -0.098093  0.445973 -0.028613  0.256372  0.029888 -0.050134   \n",
       "33340 -0.131739  0.001401  0.562934  0.016376  0.139309 -0.102250  0.267339   \n",
       "\n",
       "            7         8         9    ...       758       759       760  \\\n",
       "0      0.108739 -0.041492 -0.183632  ...  0.251432 -0.163091 -0.116690   \n",
       "1      0.026950  0.204912 -0.180139  ...  0.284122 -0.540412  0.049729   \n",
       "2      0.166499 -0.042147 -0.080027  ...  0.109118 -0.118454 -0.086577   \n",
       "3      0.107403 -0.000545 -0.192616  ...  0.225206 -0.128331 -0.235366   \n",
       "4      0.262436 -0.031409 -0.208465  ... -0.080698 -0.173298  0.043021   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33336  0.168916 -0.019692 -0.068533  ... -0.283789  0.086214  0.281354   \n",
       "33337  0.133416 -0.177918 -0.203445  ...  0.008279 -0.050800  0.085909   \n",
       "33338  0.120231  0.002479 -0.294419  ... -0.157346 -0.069377  0.014211   \n",
       "33339  0.159709  0.134332 -0.035709  ... -0.012439 -0.048856  0.105855   \n",
       "33340  0.035725  0.026772 -0.307518  ... -0.101262 -0.228757  0.278900   \n",
       "\n",
       "            761       762       763       764       765       766       767  \n",
       "0     -0.070913 -0.048259 -0.037924 -0.119490 -0.189950 -0.203662  0.074954  \n",
       "1     -0.095771 -0.203890  0.091056 -0.145161 -0.096876 -0.135908  0.060218  \n",
       "2     -0.057226  0.129574 -0.021993 -0.200361 -0.175351 -0.030066  0.166013  \n",
       "3     -0.151748 -0.025532  0.023671  0.072414 -0.086625 -0.004205  0.172869  \n",
       "4     -0.124838 -0.122861 -0.152739 -0.130289 -0.272174 -0.106081 -0.142323  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "33336  0.030180  0.144887 -0.240283 -0.269940 -0.195859 -0.090531 -0.038818  \n",
       "33337 -0.260432 -0.009710  0.090369 -0.170151 -0.095905 -0.061700 -0.136271  \n",
       "33338 -0.028526  0.019504 -0.002726 -0.118924  0.022453  0.061370  0.090219  \n",
       "33339 -0.005054  0.283087 -0.077617 -0.224488 -0.029133 -0.032732 -0.195680  \n",
       "33340  0.035723 -0.072525  0.044891 -0.259199 -0.250731 -0.136858 -0.125877  \n",
       "\n",
       "[33341 rows x 768 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertDataframe = pd.DataFrame(vectorialRepresentation)\n",
    "bertDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertData = np.array(bertDataframe)\n",
    "bertData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualização de dados com TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0          1  Target\n",
      "0     -75.484505 -21.128466       0\n",
      "1       1.098834 -35.483253       0\n",
      "2      47.738682 -36.893559       0\n",
      "3       9.928965 -70.018379       0\n",
      "4      13.938590 -21.092623       0\n",
      "...          ...        ...     ...\n",
      "33336  22.243933  56.396641       1\n",
      "33337 -31.545631 -11.026985       1\n",
      "33338 -28.561445 -19.727957       1\n",
      "33339  29.865818 -40.960075       0\n",
      "33340 -24.992823  19.384506       1\n",
      "\n",
      "[33341 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# array_red = model.fit_transform(bertDataframe)\n",
    "\n",
    "# df_tsne = pd.DataFrame(array_red)\n",
    "\n",
    "# df_tsne['Target'] = target\n",
    "# print(df_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "\n",
    "# df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "\n",
    "# plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "\n",
    "# plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bertData = np.array(bertData)\n",
    "\n",
    "bertData = bertData.reshape((bertData.shape[0],1,bertData.shape[1]))\n",
    "print(bertData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(n_splits=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 4s 3ms/step - loss: 0.3499 - accuracy: 0.8889 - val_loss: 0.1727 - val_accuracy: 0.9440 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9410 - val_loss: 0.1448 - val_accuracy: 0.9528 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9501 - val_loss: 0.1184 - val_accuracy: 0.9600 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1385 - accuracy: 0.9627 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1287 - accuracy: 0.9648 - val_loss: 0.1275 - val_accuracy: 0.9576 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0776 - accuracy: 0.9768 - val_loss: 0.0639 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0893 - accuracy: 0.9764 - val_loss: 0.0666 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.0559 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0609 - accuracy: 0.9839 - val_loss: 0.0539 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0718 - accuracy: 0.9827 - val_loss: 0.0682 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0445 - accuracy: 0.9865 - val_loss: 0.0527 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.0443 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9882 - val_loss: 0.0532 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9915 - val_loss: 0.0550 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0246 - accuracy: 0.9930 - val_loss: 0.0423 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9944 - val_loss: 0.0495 - val_accuracy: 0.9824 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0448 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0426 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0431 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.0464 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0476 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9961\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.0538 - val_accuracy: 0.9848 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0478 - val_accuracy: 0.9872 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0128 - accuracy: 0.9968 - val_loss: 0.0466 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0462 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.0456 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0448 - val_accuracy: 0.9880 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Score fold 1: loss de 0.09102164953947067; accuracy de 97.5287914276123%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3240 - accuracy: 0.8913 - val_loss: 0.1648 - val_accuracy: 0.9472 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1536 - accuracy: 0.9546 - val_loss: 0.1026 - val_accuracy: 0.9584 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1171 - accuracy: 0.9664 - val_loss: 0.0876 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9588 - val_loss: 0.2787 - val_accuracy: 0.8857 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.9499 - val_loss: 0.7121 - val_accuracy: 0.7066 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2149 - accuracy: 0.9278 - val_loss: 0.1107 - val_accuracy: 0.9560 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9687 - val_loss: 0.0864 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0813 - accuracy: 0.9749 - val_loss: 0.0697 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0574 - accuracy: 0.9812 - val_loss: 0.0616 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.0590 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0419 - accuracy: 0.9867 - val_loss: 0.0565 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.0586 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0774 - accuracy: 0.9845 - val_loss: 0.0612 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0520 - accuracy: 0.9876 - val_loss: 0.0577 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.0558 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0267 - accuracy: 0.9929 - val_loss: 0.0529 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0285 - accuracy: 0.9924 - val_loss: 0.0543 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0224 - accuracy: 0.9936 - val_loss: 0.0551 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0538 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 0.0588 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.0556 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0555 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "723/743 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9959\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0571 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0572 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0573 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0110 - accuracy: 0.9972 - val_loss: 0.0575 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "727/743 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0577 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Score fold 2: loss de 0.07085882872343063; accuracy de 97.69645929336548%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8760 - val_loss: 0.2134 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9490 - val_loss: 0.1158 - val_accuracy: 0.9544 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9676 - val_loss: 0.0948 - val_accuracy: 0.9648 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.9695 - val_loss: 0.0758 - val_accuracy: 0.9664 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0898 - accuracy: 0.9764 - val_loss: 0.0679 - val_accuracy: 0.9704 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.0603 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.9750 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1049 - accuracy: 0.9779 - val_loss: 0.2473 - val_accuracy: 0.8753 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1069 - accuracy: 0.9722 - val_loss: 0.0636 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0587 - accuracy: 0.9830 - val_loss: 0.0533 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0661 - accuracy: 0.9831 - val_loss: 0.0547 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0401 - accuracy: 0.9873 - val_loss: 0.0523 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0927 - accuracy: 0.9835 - val_loss: 0.0602 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0899 - accuracy: 0.9795 - val_loss: 0.0544 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9874 - val_loss: 0.0558 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0497 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 0.0532 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0515 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0463 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0253 - accuracy: 0.9931 - val_loss: 0.0453 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0457 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0203 - accuracy: 0.9947 - val_loss: 0.0481 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.0502 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 0.0512 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0589 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9922 - val_loss: 0.0557 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "  1/743 [..............................] - ETA: 1s - loss: 0.0097 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(bertData, targets):\n",
    "    model = keras.models.Sequential([\n",
    "        #keras.layers.Conv1D(1000, kernel_size=1, activation=\"relu\",input_shape=bertData.shape[1:]),\n",
    "        keras.layers.Conv1D(640, kernel_size=1, activation=\"relu\",input_shape=bertData.shape[1:]),\n",
    "        keras.layers.MaxPooling1D(pool_size=1),\n",
    "        keras.layers.Flatten(),\n",
    "        #keras.layers.Dense(5000, activation=\"relu\"),\n",
    "        #keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(len(set(targets)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), bertData.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(bertData[train], targets[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(bertData[test], targets[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}