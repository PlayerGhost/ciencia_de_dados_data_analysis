{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import  classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregando base de dados  pré-processada"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   email  target\n0      start date     hourahead timee  cardinall  hou...       0\n1      service long desk  price structure deal quote ...       0\n2      start date  cardinall    hourahead timee  card...       0\n3      start date     hourahead timee  cardinall  anc...       0\n4      cardinall deliverable revenue management marke...       0\n...                                                  ...     ...\n33340  bio  matrix scientific group   symbo   bmxg  p...       1\n33341   cardinall step away hot naked webcam girl liv...       1\n33342  need pill increase performance click  seroius ...       1\n33343  datee final nom       inlet hpl  eastrans  car...       0\n33344  ordinall time  offering male enhancement perfo...       1\n\n[33341 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>start date     hourahead timee  cardinall  hou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>service long desk  price structure deal quote ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>start date  cardinall    hourahead timee  card...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>start date     hourahead timee  cardinall  anc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cardinall deliverable revenue management marke...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33341</th>\n      <td>cardinall step away hot naked webcam girl liv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33342</th>\n      <td>need pill increase performance click  seroius ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33343</th>\n      <td>datee final nom       inlet hpl  eastrans  car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33344</th>\n      <td>ordinall time  offering male enhancement perfo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "target = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representação vetorial BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTextRepresentation: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": "(33341, 768)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RepresentationModel(\n",
    "    model_type=\"bert\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    use_cuda=True,\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "vectorialRepresentation = model.encode_sentences(emailsText, combine_strategy=\"mean\")\n",
    "vectorialRepresentation.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\n0     -0.159428 -0.150673  0.320971  0.112358  0.459967  0.077280  0.211645   \n1      0.071051  0.006274  0.291100 -0.089434  0.006165  0.019528  0.109083   \n2     -0.151653 -0.164808  0.471539 -0.066901  0.267297  0.017360 -0.074956   \n3     -0.091722 -0.252287  0.291835  0.154237  0.285827 -0.033622 -0.024027   \n4     -0.028974 -0.141580  0.607358  0.108809  0.400952 -0.087711 -0.108502   \n...         ...       ...       ...       ...       ...       ...       ...   \n33336 -0.201023 -0.001996  0.590600 -0.040250  0.472595  0.059005 -0.223889   \n33337  0.045905 -0.145504  0.600411  0.147447  0.138597 -0.070919  0.243944   \n33338 -0.052234 -0.027882  0.484133  0.026257  0.134670  0.035107  0.068209   \n33339 -0.226833 -0.098093  0.445973 -0.028613  0.256372  0.029888 -0.050134   \n33340 -0.131739  0.001401  0.562934  0.016376  0.139309 -0.102250  0.267339   \n\n            7         8         9    ...       758       759       760  \\\n0      0.108739 -0.041492 -0.183632  ...  0.251432 -0.163091 -0.116690   \n1      0.026950  0.204912 -0.180139  ...  0.284122 -0.540412  0.049729   \n2      0.166499 -0.042147 -0.080027  ...  0.109118 -0.118454 -0.086577   \n3      0.107403 -0.000545 -0.192616  ...  0.225206 -0.128331 -0.235366   \n4      0.262436 -0.031409 -0.208465  ... -0.080698 -0.173298  0.043021   \n...         ...       ...       ...  ...       ...       ...       ...   \n33336  0.168916 -0.019692 -0.068533  ... -0.283789  0.086214  0.281354   \n33337  0.133416 -0.177918 -0.203445  ...  0.008279 -0.050800  0.085909   \n33338  0.120231  0.002479 -0.294419  ... -0.157346 -0.069377  0.014211   \n33339  0.159709  0.134332 -0.035709  ... -0.012439 -0.048856  0.105855   \n33340  0.035725  0.026772 -0.307518  ... -0.101262 -0.228757  0.278900   \n\n            761       762       763       764       765       766       767  \n0     -0.070913 -0.048259 -0.037924 -0.119490 -0.189950 -0.203662  0.074954  \n1     -0.095771 -0.203890  0.091056 -0.145161 -0.096876 -0.135908  0.060218  \n2     -0.057226  0.129574 -0.021993 -0.200361 -0.175351 -0.030066  0.166013  \n3     -0.151748 -0.025532  0.023671  0.072414 -0.086625 -0.004205  0.172869  \n4     -0.124838 -0.122861 -0.152739 -0.130289 -0.272174 -0.106081 -0.142323  \n...         ...       ...       ...       ...       ...       ...       ...  \n33336  0.030180  0.144887 -0.240283 -0.269940 -0.195859 -0.090531 -0.038818  \n33337 -0.260432 -0.009710  0.090369 -0.170151 -0.095905 -0.061700 -0.136271  \n33338 -0.028526  0.019504 -0.002726 -0.118924  0.022453  0.061370  0.090219  \n33339 -0.005054  0.283087 -0.077617 -0.224488 -0.029133 -0.032732 -0.195680  \n33340  0.035723 -0.072525  0.044891 -0.259199 -0.250731 -0.136858 -0.125877  \n\n[33341 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.159428</td>\n      <td>-0.150673</td>\n      <td>0.320971</td>\n      <td>0.112358</td>\n      <td>0.459967</td>\n      <td>0.077280</td>\n      <td>0.211645</td>\n      <td>0.108739</td>\n      <td>-0.041492</td>\n      <td>-0.183632</td>\n      <td>...</td>\n      <td>0.251432</td>\n      <td>-0.163091</td>\n      <td>-0.116690</td>\n      <td>-0.070913</td>\n      <td>-0.048259</td>\n      <td>-0.037924</td>\n      <td>-0.119490</td>\n      <td>-0.189950</td>\n      <td>-0.203662</td>\n      <td>0.074954</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.071051</td>\n      <td>0.006274</td>\n      <td>0.291100</td>\n      <td>-0.089434</td>\n      <td>0.006165</td>\n      <td>0.019528</td>\n      <td>0.109083</td>\n      <td>0.026950</td>\n      <td>0.204912</td>\n      <td>-0.180139</td>\n      <td>...</td>\n      <td>0.284122</td>\n      <td>-0.540412</td>\n      <td>0.049729</td>\n      <td>-0.095771</td>\n      <td>-0.203890</td>\n      <td>0.091056</td>\n      <td>-0.145161</td>\n      <td>-0.096876</td>\n      <td>-0.135908</td>\n      <td>0.060218</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.151653</td>\n      <td>-0.164808</td>\n      <td>0.471539</td>\n      <td>-0.066901</td>\n      <td>0.267297</td>\n      <td>0.017360</td>\n      <td>-0.074956</td>\n      <td>0.166499</td>\n      <td>-0.042147</td>\n      <td>-0.080027</td>\n      <td>...</td>\n      <td>0.109118</td>\n      <td>-0.118454</td>\n      <td>-0.086577</td>\n      <td>-0.057226</td>\n      <td>0.129574</td>\n      <td>-0.021993</td>\n      <td>-0.200361</td>\n      <td>-0.175351</td>\n      <td>-0.030066</td>\n      <td>0.166013</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.091722</td>\n      <td>-0.252287</td>\n      <td>0.291835</td>\n      <td>0.154237</td>\n      <td>0.285827</td>\n      <td>-0.033622</td>\n      <td>-0.024027</td>\n      <td>0.107403</td>\n      <td>-0.000545</td>\n      <td>-0.192616</td>\n      <td>...</td>\n      <td>0.225206</td>\n      <td>-0.128331</td>\n      <td>-0.235366</td>\n      <td>-0.151748</td>\n      <td>-0.025532</td>\n      <td>0.023671</td>\n      <td>0.072414</td>\n      <td>-0.086625</td>\n      <td>-0.004205</td>\n      <td>0.172869</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.028974</td>\n      <td>-0.141580</td>\n      <td>0.607358</td>\n      <td>0.108809</td>\n      <td>0.400952</td>\n      <td>-0.087711</td>\n      <td>-0.108502</td>\n      <td>0.262436</td>\n      <td>-0.031409</td>\n      <td>-0.208465</td>\n      <td>...</td>\n      <td>-0.080698</td>\n      <td>-0.173298</td>\n      <td>0.043021</td>\n      <td>-0.124838</td>\n      <td>-0.122861</td>\n      <td>-0.152739</td>\n      <td>-0.130289</td>\n      <td>-0.272174</td>\n      <td>-0.106081</td>\n      <td>-0.142323</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33336</th>\n      <td>-0.201023</td>\n      <td>-0.001996</td>\n      <td>0.590600</td>\n      <td>-0.040250</td>\n      <td>0.472595</td>\n      <td>0.059005</td>\n      <td>-0.223889</td>\n      <td>0.168916</td>\n      <td>-0.019692</td>\n      <td>-0.068533</td>\n      <td>...</td>\n      <td>-0.283789</td>\n      <td>0.086214</td>\n      <td>0.281354</td>\n      <td>0.030180</td>\n      <td>0.144887</td>\n      <td>-0.240283</td>\n      <td>-0.269940</td>\n      <td>-0.195859</td>\n      <td>-0.090531</td>\n      <td>-0.038818</td>\n    </tr>\n    <tr>\n      <th>33337</th>\n      <td>0.045905</td>\n      <td>-0.145504</td>\n      <td>0.600411</td>\n      <td>0.147447</td>\n      <td>0.138597</td>\n      <td>-0.070919</td>\n      <td>0.243944</td>\n      <td>0.133416</td>\n      <td>-0.177918</td>\n      <td>-0.203445</td>\n      <td>...</td>\n      <td>0.008279</td>\n      <td>-0.050800</td>\n      <td>0.085909</td>\n      <td>-0.260432</td>\n      <td>-0.009710</td>\n      <td>0.090369</td>\n      <td>-0.170151</td>\n      <td>-0.095905</td>\n      <td>-0.061700</td>\n      <td>-0.136271</td>\n    </tr>\n    <tr>\n      <th>33338</th>\n      <td>-0.052234</td>\n      <td>-0.027882</td>\n      <td>0.484133</td>\n      <td>0.026257</td>\n      <td>0.134670</td>\n      <td>0.035107</td>\n      <td>0.068209</td>\n      <td>0.120231</td>\n      <td>0.002479</td>\n      <td>-0.294419</td>\n      <td>...</td>\n      <td>-0.157346</td>\n      <td>-0.069377</td>\n      <td>0.014211</td>\n      <td>-0.028526</td>\n      <td>0.019504</td>\n      <td>-0.002726</td>\n      <td>-0.118924</td>\n      <td>0.022453</td>\n      <td>0.061370</td>\n      <td>0.090219</td>\n    </tr>\n    <tr>\n      <th>33339</th>\n      <td>-0.226833</td>\n      <td>-0.098093</td>\n      <td>0.445973</td>\n      <td>-0.028613</td>\n      <td>0.256372</td>\n      <td>0.029888</td>\n      <td>-0.050134</td>\n      <td>0.159709</td>\n      <td>0.134332</td>\n      <td>-0.035709</td>\n      <td>...</td>\n      <td>-0.012439</td>\n      <td>-0.048856</td>\n      <td>0.105855</td>\n      <td>-0.005054</td>\n      <td>0.283087</td>\n      <td>-0.077617</td>\n      <td>-0.224488</td>\n      <td>-0.029133</td>\n      <td>-0.032732</td>\n      <td>-0.195680</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>-0.131739</td>\n      <td>0.001401</td>\n      <td>0.562934</td>\n      <td>0.016376</td>\n      <td>0.139309</td>\n      <td>-0.102250</td>\n      <td>0.267339</td>\n      <td>0.035725</td>\n      <td>0.026772</td>\n      <td>-0.307518</td>\n      <td>...</td>\n      <td>-0.101262</td>\n      <td>-0.228757</td>\n      <td>0.278900</td>\n      <td>0.035723</td>\n      <td>-0.072525</td>\n      <td>0.044891</td>\n      <td>-0.259199</td>\n      <td>-0.250731</td>\n      <td>-0.136858</td>\n      <td>-0.125877</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertDataframe = pd.DataFrame(vectorialRepresentation)\n",
    "bertDataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.15942807, -0.15067285,  0.3209715 , ..., -0.18995005,\n        -0.20366167,  0.07495396],\n       [ 0.07105102,  0.00627373,  0.2911002 , ..., -0.09687632,\n        -0.13590766,  0.06021798],\n       [-0.15165325, -0.1648083 ,  0.47153878, ..., -0.17535143,\n        -0.03006648,  0.16601259],\n       ...,\n       [-0.05223386, -0.02788216,  0.48413256, ...,  0.0224532 ,\n         0.06136988,  0.09021917],\n       [-0.22683339, -0.09809302,  0.44597265, ..., -0.02913298,\n        -0.03273212, -0.19567987],\n       [-0.13173941,  0.0014008 ,  0.56293374, ..., -0.2507311 ,\n        -0.13685788, -0.12587653]], dtype=float32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertData = np.array(bertDataframe)\n",
    "bertData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33341, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "bertData = bertData.reshape((bertData.shape[0], 1, bertData.shape[1]))\n",
    "\n",
    "print(bertData.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualização de dados com TSNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# array_red = model.fit_transform(bertDataframe)\n",
    "#\n",
    "# df_tsne = pd.DataFrame(array_red)\n",
    "#\n",
    "# df_tsne['Target'] = target\n",
    "# print(df_tsne)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "#\n",
    "# df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "#\n",
    "# plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "#\n",
    "# plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "#\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "#\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validação"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6935 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5065 - val_loss: 0.6939 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6938 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6939 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5064 - val_loss: 0.6932 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5061 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5049 - val_loss: 0.6937 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "741/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5051\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6932 - val_accuracy: 0.4836 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6933 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 19/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5051\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-03\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5051 - val_loss: 0.6934 - val_accuracy: 0.4836 - lr: 1.0000e-04\n",
      "Score fold 1: loss de 0.6930419206619263; accuracy de 50.923705101013184%\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5087 - val_loss: 0.6930 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6930 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6930 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5068 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5075\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6930 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5072\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5070\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 29/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5071\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-05\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5071\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-06\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5071\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-07\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 45/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 46/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 50/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5068\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-08\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 52/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 53/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 54/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 55/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 56/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 57/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5071\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-09\n",
      "Epoch 58/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-10\n",
      "Epoch 59/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-10\n",
      "Epoch 60/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-10\n",
      "Epoch 61/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-10\n",
      "Epoch 62/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5092 - lr: 1.0000e-10\n",
      "Score fold 2: loss de 0.6931531429290771; accuracy de 49.97000694274902%\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5068 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6931 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6937 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5055 - val_loss: 0.6934 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5062 - val_loss: 0.6932 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6936 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6936 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5074\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5075 - val_loss: 0.6934 - val_accuracy: 0.4932 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5060\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4932 - lr: 1.0000e-04\n",
      "Score fold 3: loss de 0.6931565999984741; accuracy de 49.454107880592346%\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 4.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5041 - val_loss: 0.6937 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6932 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5062 - val_loss: 0.6935 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5057 - val_loss: 0.6937 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6938 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5060 - val_loss: 0.6938 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5063 - val_loss: 0.6938 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5075\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5076 - val_loss: 0.6933 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6933 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5055\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-03\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5057 - val_loss: 0.6934 - val_accuracy: 0.4868 - lr: 1.0000e-04\n",
      "Score fold 4: loss de 0.69305419921875; accuracy de 50.68986415863037%\n"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(bertData, target):\n",
    "    model = keras.models.Sequential([\n",
    "        # keras.layers.LSTM(50, activation='tanh',recurrent_activation='sigmoid',input_shape=[1, bertData.shape[2]]),\n",
    "\n",
    "        ########## Stacked LSTM\n",
    "        keras.layers.LSTM(784, activation='relu', return_sequences=True, input_shape=[1, bertData.shape[2]]),\n",
    "        keras.layers.LSTM(250, activation='relu', return_sequences=True),\n",
    "        keras.layers.LSTM(90, activation='relu', return_sequences=True),\n",
    "        keras.layers.LSTM(30, activation='relu'),\n",
    "\n",
    "        keras.layers.Dense(len(set(target)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(bertData[train], target[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(bertData[test], target[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Score de cada fold:\n",
      "****************************************************\n",
      "--> Fold 1: Loss: 0.6930419206619263 ; Accuracy: 50.923705101013184%\n",
      "****************************************************\n",
      "--> Fold 2: Loss: 0.6931531429290771 ; Accuracy: 49.97000694274902%\n",
      "****************************************************\n",
      "--> Fold 3: Loss: 0.6931565999984741 ; Accuracy: 49.454107880592346%\n",
      "****************************************************\n",
      "--> Fold 4: Loss: 0.69305419921875 ; Accuracy: 50.68986415863037%\n",
      "****************************************************\n",
      "Média de accuracy das folds:\n",
      "--> Accuracy: 50.25942102074623 (+- 0.5828474063799075)\n",
      "--> Loss: 0.6931014657020569\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#categories = [\"Ham\", \"Spam\"]\n",
    "#\n",
    "#skplt.metrics.plot_confusion_matrix(\n",
    "#    [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#    title=\"Confusion Matrix\",\n",
    "#    cmap=\"Purples\",\n",
    "#    hide_zeros=True,\n",
    "#    figsize=(5,5)\n",
    "#)\n",
    "#\n",
    "#plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# skplt.metrics.plot_confusion_matrix(\n",
    "#     [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#     normalize=True,\n",
    "#     title=\"Confusion Matrix\",\n",
    "#     cmap=\"Purples\",\n",
    "#     hide_zeros=True,\n",
    "#     figsize=(5,5)\n",
    "# )\n",
    "#\n",
    "# plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}