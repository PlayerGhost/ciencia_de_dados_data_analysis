{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   email  target\n0      start date     hourahead timee  cardinall  hou...       0\n1      service long desk  price structure deal quote ...       0\n2      start date  cardinall    hourahead timee  card...       0\n3      start date     hourahead timee  cardinall  anc...       0\n4      cardinall deliverable revenue management marke...       0\n...                                                  ...     ...\n33340  bio  matrix scientific group   symbo   bmxg  p...       1\n33341   cardinall step away hot naked webcam girl liv...       1\n33342  need pill increase performance click  seroius ...       1\n33343  datee final nom       inlet hpl  eastrans  car...       0\n33344  ordinall time  offering male enhancement perfo...       1\n\n[33341 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>start date     hourahead timee  cardinall  hou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>service long desk  price structure deal quote ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>start date  cardinall    hourahead timee  card...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>start date     hourahead timee  cardinall  anc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cardinall deliverable revenue management marke...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33341</th>\n      <td>cardinall step away hot naked webcam girl liv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33342</th>\n      <td>need pill increase performance click  seroius ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33343</th>\n      <td>datee final nom       inlet hpl  eastrans  car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33344</th>\n      <td>ordinall time  offering male enhancement perfo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "target = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# skplt.metrics.plot_confusion_matrix(\n",
    "#     [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#     normalize=True,\n",
    "#     title=\"Confusion Matrix\",\n",
    "#     cmap=\"Purples\",\n",
    "#     hide_zeros=True,\n",
    "#     figsize=(5,5)\n",
    "# )\n",
    "#\n",
    "# plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Carregando base de dados  pré-processada"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representação vetorial GPT2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2ForTextRepresentation: ['h.8.ln_2.weight', 'h.11.attn.c_proj.bias', 'h.8.attn.c_proj.bias', 'h.4.attn.bias', 'h.4.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.2.ln_2.weight', 'h.0.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.7.mlp.c_proj.weight', 'h.7.ln_2.bias', 'h.4.ln_2.bias', 'h.2.ln_1.weight', 'h.10.ln_1.bias', 'h.6.attn.c_attn.bias', 'h.1.mlp.c_proj.weight', 'h.7.ln_1.weight', 'h.4.ln_1.weight', 'h.1.attn.c_proj.weight', 'h.4.attn.c_proj.weight', 'h.11.ln_1.bias', 'h.0.attn.c_proj.weight', 'h.2.attn.bias', 'h.2.attn.c_attn.weight', 'h.2.attn.c_attn.bias', 'h.8.attn.c_attn.weight', 'h.2.mlp.c_proj.weight', 'h.8.mlp.c_fc.bias', 'h.2.mlp.c_fc.weight', 'h.6.ln_2.bias', 'h.4.mlp.c_proj.weight', 'h.9.ln_1.bias', 'h.9.mlp.c_proj.weight', 'h.11.attn.bias', 'h.9.attn.c_attn.bias', 'h.0.mlp.c_fc.bias', 'h.1.attn.c_proj.bias', 'h.10.ln_2.weight', 'h.10.attn.c_proj.weight', 'h.10.attn.c_attn.weight', 'h.7.ln_1.bias', 'h.0.ln_2.weight', 'h.5.attn.c_proj.weight', 'h.4.attn.c_attn.weight', 'h.8.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.8.mlp.c_proj.bias', 'h.5.mlp.c_fc.bias', 'h.9.ln_2.bias', 'h.9.ln_1.weight', 'h.5.attn.c_proj.bias', 'h.9.attn.bias', 'h.6.attn.bias', 'h.0.attn.c_attn.weight', 'h.4.mlp.c_fc.weight', 'h.6.ln_1.bias', 'h.5.attn.bias', 'h.8.mlp.c_fc.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.bias', 'h.6.attn.c_attn.weight', 'h.7.mlp.c_proj.bias', 'h.3.attn.bias', 'h.0.attn.bias', 'h.0.mlp.c_proj.bias', 'h.11.mlp.c_fc.bias', 'h.3.ln_2.weight', 'h.7.attn.c_attn.weight', 'h.11.mlp.c_proj.weight', 'h.0.ln_1.weight', 'h.9.attn.c_proj.bias', 'h.4.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.6.attn.c_proj.weight', 'h.10.mlp.c_fc.bias', 'h.7.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.11.ln_1.weight', 'h.3.attn.c_attn.bias', 'h.1.ln_2.bias', 'h.5.mlp.c_fc.weight', 'h.10.ln_2.bias', 'h.6.mlp.c_fc.weight', 'h.1.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.9.ln_2.weight', 'h.8.attn.bias', 'h.10.attn.bias', 'h.11.mlp.c_proj.bias', 'h.1.mlp.c_fc.bias', 'h.6.mlp.c_proj.weight', 'h.0.attn.c_proj.bias', 'h.8.attn.c_attn.bias', 'h.9.mlp.c_proj.bias', 'wpe.weight', 'h.7.ln_2.weight', 'h.6.mlp.c_fc.bias', 'h.9.mlp.c_fc.weight', 'h.6.ln_1.weight', 'h.5.ln_1.weight', 'wte.weight', 'h.8.ln_2.bias', 'h.11.attn.c_attn.weight', 'h.3.mlp.c_fc.bias', 'h.11.attn.c_proj.weight', 'h.11.mlp.c_fc.weight', 'h.8.ln_1.bias', 'h.2.attn.c_proj.bias', 'h.5.attn.c_attn.bias', 'h.6.attn.c_proj.bias', 'h.1.ln_1.bias', 'h.11.attn.c_attn.bias', 'h.3.ln_2.bias', 'h.10.ln_1.weight', 'h.10.attn.c_proj.bias', 'h.3.attn.c_proj.bias', 'h.7.attn.bias', 'h.2.attn.c_proj.weight', 'h.1.mlp.c_fc.weight', 'h.10.attn.c_attn.bias', 'h.8.attn.c_proj.weight', 'h.10.mlp.c_proj.weight', 'h.3.attn.c_proj.weight', 'h.9.attn.c_attn.weight', 'h.6.mlp.c_proj.bias', 'h.2.mlp.c_proj.bias', 'h.3.ln_1.bias', 'h.3.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'ln_f.weight', 'h.10.mlp.c_proj.bias', 'h.3.attn.c_attn.weight', 'h.3.mlp.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.2.ln_2.bias', 'h.0.ln_1.bias', 'h.11.ln_2.weight', 'h.4.mlp.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.6.ln_2.weight', 'h.5.attn.c_attn.weight', 'h.5.mlp.c_proj.bias', 'h.0.ln_2.bias', 'h.5.ln_1.bias', 'h.4.ln_1.bias', 'h.7.mlp.c_fc.bias', 'h.11.ln_2.bias', 'h.2.ln_1.bias', 'h.1.ln_1.weight', 'h.4.attn.c_proj.bias', 'h.10.mlp.c_fc.weight', 'ln_f.bias', 'h.5.ln_2.weight', 'h.4.ln_2.weight', 'h.9.mlp.c_fc.bias', 'h.3.mlp.c_fc.weight', 'h.0.attn.c_attn.bias', 'h.7.attn.c_attn.bias', 'h.5.ln_2.bias']\n",
      "- This IS expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForTextRepresentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForTextRepresentation were not initialized from the model checkpoint at gpt2 and are newly initialized: ['gpt2.h.7.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_proj.bias', 'gpt2.h.6.attn.c_attn.bias', 'gpt2.h.10.attn.c_proj.bias', 'gpt2.h.7.mlp.c_proj.bias', 'gpt2.h.11.ln_1.weight', 'gpt2.h.11.attn.c_attn.bias', 'gpt2.h.3.mlp.c_fc.weight', 'gpt2.h.6.ln_2.bias', 'gpt2.h.7.attn.bias', 'gpt2.h.9.attn.masked_bias', 'gpt2.h.0.ln_1.bias', 'gpt2.h.4.mlp.c_fc.weight', 'gpt2.h.2.ln_1.bias', 'gpt2.h.1.mlp.c_fc.bias', 'gpt2.h.9.mlp.c_fc.weight', 'gpt2.h.5.attn.bias', 'gpt2.h.1.attn.c_attn.weight', 'gpt2.h.8.attn.c_proj.weight', 'gpt2.h.5.attn.c_attn.weight', 'gpt2.h.4.attn.bias', 'gpt2.h.0.mlp.c_fc.weight', 'gpt2.ln_f.weight', 'gpt2.h.5.ln_2.weight', 'gpt2.h.9.mlp.c_proj.weight', 'gpt2.h.2.ln_2.bias', 'gpt2.h.1.ln_2.bias', 'gpt2.h.6.attn.c_proj.bias', 'gpt2.h.3.ln_2.bias', 'gpt2.h.0.mlp.c_proj.weight', 'gpt2.h.3.ln_1.bias', 'gpt2.h.1.attn.bias', 'gpt2.h.4.ln_1.bias', 'gpt2.h.4.attn.masked_bias', 'gpt2.h.7.attn.c_attn.bias', 'gpt2.h.2.attn.c_proj.bias', 'gpt2.h.10.attn.bias', 'gpt2.h.10.ln_2.weight', 'gpt2.h.3.attn.c_attn.bias', 'gpt2.h.3.attn.bias', 'gpt2.h.10.attn.c_proj.weight', 'gpt2.h.10.attn.c_attn.bias', 'gpt2.h.8.mlp.c_fc.bias', 'gpt2.h.8.attn.c_attn.weight', 'gpt2.h.3.mlp.c_proj.weight', 'gpt2.h.1.mlp.c_fc.weight', 'gpt2.h.7.mlp.c_fc.weight', 'gpt2.h.9.attn.c_proj.bias', 'gpt2.h.10.mlp.c_proj.weight', 'gpt2.h.0.attn.c_attn.weight', 'gpt2.h.6.mlp.c_proj.bias', 'gpt2.h.8.ln_1.bias', 'gpt2.h.7.attn.c_proj.bias', 'gpt2.h.9.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_proj.bias', 'gpt2.h.5.ln_1.weight', 'gpt2.h.2.attn.c_attn.bias', 'gpt2.h.9.ln_2.weight', 'gpt2.h.8.ln_2.weight', 'gpt2.h.10.mlp.c_fc.bias', 'gpt2.h.11.mlp.c_proj.weight', 'gpt2.h.3.mlp.c_fc.bias', 'gpt2.h.5.mlp.c_proj.bias', 'gpt2.h.1.attn.masked_bias', 'gpt2.h.4.mlp.c_fc.bias', 'gpt2.h.10.mlp.c_fc.weight', 'gpt2.h.4.attn.c_attn.bias', 'gpt2.h.11.mlp.c_fc.weight', 'gpt2.h.9.attn.bias', 'gpt2.h.2.attn.c_attn.weight', 'gpt2.h.11.ln_2.bias', 'gpt2.h.1.attn.c_proj.bias', 'gpt2.wpe.weight', 'gpt2.h.2.ln_2.weight', 'gpt2.h.5.attn.c_proj.bias', 'gpt2.h.7.attn.c_proj.weight', 'gpt2.h.11.attn.c_proj.bias', 'gpt2.h.4.attn.c_proj.bias', 'gpt2.h.9.ln_1.bias', 'gpt2.h.2.attn.masked_bias', 'gpt2.h.1.attn.c_proj.weight', 'gpt2.h.6.ln_1.bias', 'gpt2.h.10.attn.c_attn.weight', 'gpt2.h.0.attn.c_proj.bias', 'gpt2.h.4.mlp.c_proj.bias', 'gpt2.h.6.attn.c_proj.weight', 'gpt2.h.10.mlp.c_proj.bias', 'gpt2.h.0.attn.bias', 'gpt2.h.4.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_proj.weight', 'gpt2.h.1.ln_1.bias', 'gpt2.h.2.ln_1.weight', 'gpt2.h.6.attn.c_attn.weight', 'gpt2.h.6.ln_2.weight', 'gpt2.h.8.mlp.c_fc.weight', 'gpt2.h.1.ln_2.weight', 'gpt2.h.8.mlp.c_proj.bias', 'gpt2.h.3.attn.c_proj.bias', 'gpt2.h.7.ln_2.weight', 'gpt2.h.9.attn.c_proj.weight', 'gpt2.h.10.ln_2.bias', 'gpt2.h.3.attn.masked_bias', 'gpt2.h.11.ln_1.bias', 'gpt2.h.7.ln_2.bias', 'gpt2.h.11.attn.bias', 'gpt2.h.7.ln_1.bias', 'gpt2.h.3.mlp.c_proj.bias', 'gpt2.h.9.attn.c_attn.weight', 'gpt2.h.7.attn.c_attn.weight', 'gpt2.h.0.mlp.c_fc.bias', 'gpt2.ln_f.bias', 'gpt2.h.4.attn.c_attn.weight', 'gpt2.h.3.attn.c_attn.weight', 'gpt2.h.0.ln_1.weight', 'gpt2.h.8.mlp.c_proj.weight', 'gpt2.h.11.attn.c_proj.weight', 'gpt2.h.6.ln_1.weight', 'gpt2.h.5.ln_1.bias', 'gpt2.h.5.attn.masked_bias', 'gpt2.h.10.ln_1.bias', 'gpt2.h.5.attn.c_attn.bias', 'gpt2.h.6.attn.masked_bias', 'gpt2.h.9.ln_1.weight', 'gpt2.h.0.attn.c_attn.bias', 'gpt2.h.2.mlp.c_proj.weight', 'gpt2.h.6.mlp.c_fc.weight', 'gpt2.h.3.attn.c_proj.weight', 'gpt2.h.0.attn.c_proj.weight', 'gpt2.h.8.ln_1.weight', 'gpt2.h.11.attn.masked_bias', 'gpt2.h.9.attn.c_attn.bias', 'gpt2.h.4.attn.c_proj.weight', 'gpt2.h.5.ln_2.bias', 'gpt2.h.8.attn.c_proj.bias', 'gpt2.h.1.attn.c_attn.bias', 'gpt2.h.4.ln_2.weight', 'gpt2.h.7.ln_1.weight', 'gpt2.h.2.attn.bias', 'gpt2.h.3.ln_1.weight', 'gpt2.h.4.ln_2.bias', 'gpt2.h.7.attn.masked_bias', 'gpt2.h.8.attn.masked_bias', 'gpt2.h.2.attn.c_proj.weight', 'gpt2.h.9.mlp.c_proj.bias', 'gpt2.h.8.attn.bias', 'gpt2.h.0.ln_2.weight', 'gpt2.h.8.ln_2.bias', 'gpt2.h.0.mlp.c_proj.bias', 'gpt2.h.1.mlp.c_proj.bias', 'gpt2.h.3.ln_2.weight', 'gpt2.h.10.ln_1.weight', 'gpt2.h.5.mlp.c_fc.bias', 'gpt2.h.2.mlp.c_fc.weight', 'gpt2.h.2.mlp.c_fc.bias', 'gpt2.h.11.attn.c_attn.weight', 'gpt2.h.6.mlp.c_fc.bias', 'gpt2.h.8.attn.c_attn.bias', 'gpt2.h.9.ln_2.bias', 'gpt2.h.11.mlp.c_fc.bias', 'gpt2.h.1.mlp.c_proj.weight', 'gpt2.h.1.ln_1.weight', 'gpt2.h.10.attn.masked_bias', 'gpt2.h.6.attn.bias', 'gpt2.h.7.mlp.c_proj.weight', 'gpt2.h.6.mlp.c_proj.weight', 'gpt2.h.5.mlp.c_fc.weight', 'gpt2.h.4.ln_1.weight', 'gpt2.wte.weight', 'gpt2.h.0.ln_2.bias', 'gpt2.h.11.ln_2.weight', 'gpt2.h.0.attn.masked_bias', 'gpt2.h.5.attn.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "(33341, 768)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RepresentationModel(\n",
    "    model_type=\"gpt2\",\n",
    "    model_name=\"gpt2\",\n",
    "    use_cuda=True,\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "vectorialRepresentation = model.encode_sentences(emailsText, combine_strategy=\"mean\")\n",
    "vectorialRepresentation.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6    \\\n0      0.754425  0.789269 -1.293806  0.634251  1.938044 -0.569781  0.377824   \n1      0.321773 -0.233847 -0.269172 -0.588057  1.768485 -0.280607  1.047430   \n2      0.896560  0.546716 -1.677532  1.030409  1.285654 -0.619201  0.424408   \n3      0.792714  0.782668 -1.189025  0.778011  1.444595 -0.733423  0.294439   \n4      0.332344  0.362971 -0.405684  0.397749  0.358304 -0.958279  0.946837   \n...         ...       ...       ...       ...       ...       ...       ...   \n33336  0.511861  0.832514 -1.399648  1.851290  1.984925 -1.036926  0.451013   \n33337  0.977407  0.266629 -1.604069  1.096016  1.205035 -0.459866  0.830633   \n33338  0.664014  0.424617 -0.739346  1.113884  0.884731 -0.950039  1.202816   \n33339  0.850908  0.085265 -1.509501  1.247984  1.079362 -0.508961  0.856407   \n33340  0.986957  0.213570 -1.116312  1.811600  1.749741 -0.583824  0.868704   \n\n            7         8         9    ...       758       759       760  \\\n0      1.340950 -1.166817  1.011252  ... -0.191962  1.058731 -2.277187   \n1     -0.086234 -0.406580  0.223166  ... -1.130275  1.211152 -1.572865   \n2      1.089380 -0.736149  1.082911  ...  0.223894  0.580283 -2.416588   \n3      1.249979 -1.161031  0.905683  ...  0.020335  0.639204 -2.571596   \n4      1.157139 -0.058189  1.650188  ... -0.706734  1.681758 -1.429077   \n...         ...       ...       ...  ...       ...       ...       ...   \n33336  0.838663 -0.814666  0.708901  ... -0.188347  0.723476 -2.137387   \n33337  0.287916 -0.410020  0.202285  ... -0.549093  0.824621 -2.167791   \n33338  0.975913  0.153484  0.932813  ... -0.160917 -0.168504 -2.553533   \n33339  0.817574 -1.140860  0.928794  ... -0.336332  0.475237 -1.727519   \n33340  1.081038  0.159576  0.880307  ... -0.055041  0.952323 -2.069260   \n\n            761       762       763       764       765       766       767  \n0     -0.253051 -0.489028  0.859364  1.936644  1.610118  0.025869 -0.955793  \n1     -0.519615  1.579095  0.691370  1.846132 -0.937626 -0.170233  0.364428  \n2     -0.078011 -0.320236  0.411358  1.458600  0.928713 -0.239540 -0.354970  \n3     -0.137720 -0.178574  0.334022  1.673328  1.474140 -0.191565 -0.190749  \n4      0.629436  0.213067  1.147849  0.973108 -0.432340 -0.702642 -0.282397  \n...         ...       ...       ...       ...       ...       ...       ...  \n33336 -0.200568  0.172080 -0.472850  1.178394  1.253184  0.021227 -0.309527  \n33337  1.013140  0.604684  0.401232  1.207484 -0.720511 -0.354162 -0.222643  \n33338  0.107064  0.665774 -0.164771  1.526442  0.563803 -0.384363  0.060814  \n33339 -0.014860 -0.100887 -0.182419  1.447629  0.743375 -0.085994 -0.830510  \n33340  0.678276 -0.194923  0.611555  1.518976 -1.026872 -0.192891 -0.377919  \n\n[33341 rows x 768 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>758</th>\n      <th>759</th>\n      <th>760</th>\n      <th>761</th>\n      <th>762</th>\n      <th>763</th>\n      <th>764</th>\n      <th>765</th>\n      <th>766</th>\n      <th>767</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.754425</td>\n      <td>0.789269</td>\n      <td>-1.293806</td>\n      <td>0.634251</td>\n      <td>1.938044</td>\n      <td>-0.569781</td>\n      <td>0.377824</td>\n      <td>1.340950</td>\n      <td>-1.166817</td>\n      <td>1.011252</td>\n      <td>...</td>\n      <td>-0.191962</td>\n      <td>1.058731</td>\n      <td>-2.277187</td>\n      <td>-0.253051</td>\n      <td>-0.489028</td>\n      <td>0.859364</td>\n      <td>1.936644</td>\n      <td>1.610118</td>\n      <td>0.025869</td>\n      <td>-0.955793</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.321773</td>\n      <td>-0.233847</td>\n      <td>-0.269172</td>\n      <td>-0.588057</td>\n      <td>1.768485</td>\n      <td>-0.280607</td>\n      <td>1.047430</td>\n      <td>-0.086234</td>\n      <td>-0.406580</td>\n      <td>0.223166</td>\n      <td>...</td>\n      <td>-1.130275</td>\n      <td>1.211152</td>\n      <td>-1.572865</td>\n      <td>-0.519615</td>\n      <td>1.579095</td>\n      <td>0.691370</td>\n      <td>1.846132</td>\n      <td>-0.937626</td>\n      <td>-0.170233</td>\n      <td>0.364428</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.896560</td>\n      <td>0.546716</td>\n      <td>-1.677532</td>\n      <td>1.030409</td>\n      <td>1.285654</td>\n      <td>-0.619201</td>\n      <td>0.424408</td>\n      <td>1.089380</td>\n      <td>-0.736149</td>\n      <td>1.082911</td>\n      <td>...</td>\n      <td>0.223894</td>\n      <td>0.580283</td>\n      <td>-2.416588</td>\n      <td>-0.078011</td>\n      <td>-0.320236</td>\n      <td>0.411358</td>\n      <td>1.458600</td>\n      <td>0.928713</td>\n      <td>-0.239540</td>\n      <td>-0.354970</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.792714</td>\n      <td>0.782668</td>\n      <td>-1.189025</td>\n      <td>0.778011</td>\n      <td>1.444595</td>\n      <td>-0.733423</td>\n      <td>0.294439</td>\n      <td>1.249979</td>\n      <td>-1.161031</td>\n      <td>0.905683</td>\n      <td>...</td>\n      <td>0.020335</td>\n      <td>0.639204</td>\n      <td>-2.571596</td>\n      <td>-0.137720</td>\n      <td>-0.178574</td>\n      <td>0.334022</td>\n      <td>1.673328</td>\n      <td>1.474140</td>\n      <td>-0.191565</td>\n      <td>-0.190749</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.332344</td>\n      <td>0.362971</td>\n      <td>-0.405684</td>\n      <td>0.397749</td>\n      <td>0.358304</td>\n      <td>-0.958279</td>\n      <td>0.946837</td>\n      <td>1.157139</td>\n      <td>-0.058189</td>\n      <td>1.650188</td>\n      <td>...</td>\n      <td>-0.706734</td>\n      <td>1.681758</td>\n      <td>-1.429077</td>\n      <td>0.629436</td>\n      <td>0.213067</td>\n      <td>1.147849</td>\n      <td>0.973108</td>\n      <td>-0.432340</td>\n      <td>-0.702642</td>\n      <td>-0.282397</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33336</th>\n      <td>0.511861</td>\n      <td>0.832514</td>\n      <td>-1.399648</td>\n      <td>1.851290</td>\n      <td>1.984925</td>\n      <td>-1.036926</td>\n      <td>0.451013</td>\n      <td>0.838663</td>\n      <td>-0.814666</td>\n      <td>0.708901</td>\n      <td>...</td>\n      <td>-0.188347</td>\n      <td>0.723476</td>\n      <td>-2.137387</td>\n      <td>-0.200568</td>\n      <td>0.172080</td>\n      <td>-0.472850</td>\n      <td>1.178394</td>\n      <td>1.253184</td>\n      <td>0.021227</td>\n      <td>-0.309527</td>\n    </tr>\n    <tr>\n      <th>33337</th>\n      <td>0.977407</td>\n      <td>0.266629</td>\n      <td>-1.604069</td>\n      <td>1.096016</td>\n      <td>1.205035</td>\n      <td>-0.459866</td>\n      <td>0.830633</td>\n      <td>0.287916</td>\n      <td>-0.410020</td>\n      <td>0.202285</td>\n      <td>...</td>\n      <td>-0.549093</td>\n      <td>0.824621</td>\n      <td>-2.167791</td>\n      <td>1.013140</td>\n      <td>0.604684</td>\n      <td>0.401232</td>\n      <td>1.207484</td>\n      <td>-0.720511</td>\n      <td>-0.354162</td>\n      <td>-0.222643</td>\n    </tr>\n    <tr>\n      <th>33338</th>\n      <td>0.664014</td>\n      <td>0.424617</td>\n      <td>-0.739346</td>\n      <td>1.113884</td>\n      <td>0.884731</td>\n      <td>-0.950039</td>\n      <td>1.202816</td>\n      <td>0.975913</td>\n      <td>0.153484</td>\n      <td>0.932813</td>\n      <td>...</td>\n      <td>-0.160917</td>\n      <td>-0.168504</td>\n      <td>-2.553533</td>\n      <td>0.107064</td>\n      <td>0.665774</td>\n      <td>-0.164771</td>\n      <td>1.526442</td>\n      <td>0.563803</td>\n      <td>-0.384363</td>\n      <td>0.060814</td>\n    </tr>\n    <tr>\n      <th>33339</th>\n      <td>0.850908</td>\n      <td>0.085265</td>\n      <td>-1.509501</td>\n      <td>1.247984</td>\n      <td>1.079362</td>\n      <td>-0.508961</td>\n      <td>0.856407</td>\n      <td>0.817574</td>\n      <td>-1.140860</td>\n      <td>0.928794</td>\n      <td>...</td>\n      <td>-0.336332</td>\n      <td>0.475237</td>\n      <td>-1.727519</td>\n      <td>-0.014860</td>\n      <td>-0.100887</td>\n      <td>-0.182419</td>\n      <td>1.447629</td>\n      <td>0.743375</td>\n      <td>-0.085994</td>\n      <td>-0.830510</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>0.986957</td>\n      <td>0.213570</td>\n      <td>-1.116312</td>\n      <td>1.811600</td>\n      <td>1.749741</td>\n      <td>-0.583824</td>\n      <td>0.868704</td>\n      <td>1.081038</td>\n      <td>0.159576</td>\n      <td>0.880307</td>\n      <td>...</td>\n      <td>-0.055041</td>\n      <td>0.952323</td>\n      <td>-2.069260</td>\n      <td>0.678276</td>\n      <td>-0.194923</td>\n      <td>0.611555</td>\n      <td>1.518976</td>\n      <td>-1.026872</td>\n      <td>-0.192891</td>\n      <td>-0.377919</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 768 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2Dataframe = pd.DataFrame(vectorialRepresentation)\n",
    "gpt2Dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.7544254 ,  0.78926885, -1.2938058 , ...,  1.6101176 ,\n         0.02586938, -0.95579314],\n       [ 0.3217735 , -0.23384738, -0.26917163, ..., -0.93762565,\n        -0.17023289,  0.36442804],\n       [ 0.8965603 ,  0.54671633, -1.6775317 , ...,  0.9287129 ,\n        -0.23953958, -0.35496992],\n       ...,\n       [ 0.66401446,  0.42461717, -0.73934567, ...,  0.5638028 ,\n        -0.38436264,  0.06081423],\n       [ 0.8509083 ,  0.08526485, -1.509501  , ...,  0.7433747 ,\n        -0.0859936 , -0.83051026],\n       [ 0.9869572 ,  0.21356997, -1.1163118 , ..., -1.0268719 ,\n        -0.1928907 , -0.37791866]], dtype=float32)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2Data = np.array(gpt2Dataframe)\n",
    "gpt2Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33341, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "gpt2Data = gpt2Data.reshape((gpt2Data.shape[0], 1, gpt2Data.shape[1]))\n",
    "\n",
    "print(gpt2Data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualização de dados com TSNE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# array_red = model.fit_transform(gpt2Dataframe)\n",
    "#\n",
    "# df_tsne = pd.DataFrame(array_red)\n",
    "#\n",
    "# df_tsne['Target'] = target\n",
    "# print(df_tsne)\n",
    "# df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "#\n",
    "# df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "#\n",
    "# plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "#\n",
    "# plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "#\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "#\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Validação"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5021 - val_loss: 0.6932 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6930 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.6935 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.6930 - accuracy: 0.5056 - val_loss: 0.6931 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.6930 - accuracy: 0.5065 - val_loss: 0.6934 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5044 - val_loss: 0.6930 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5101 - val_loss: 0.6931 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5060 - val_loss: 0.6931 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5060 - val_loss: 0.6928 - val_accuracy: 0.5084 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5091 - val_loss: 0.6929 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6928 - accuracy: 0.5170 - val_loss: 0.6929 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6928 - accuracy: 0.5176 - val_loss: 0.6929 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6927 - accuracy: 0.5162 - val_loss: 0.6927 - val_accuracy: 0.5755 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6927 - accuracy: 0.5164 - val_loss: 0.6929 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6926 - accuracy: 0.5178 - val_loss: 0.6928 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6925 - accuracy: 0.5147 - val_loss: 0.6924 - val_accuracy: 0.6427 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6924 - accuracy: 0.5223 - val_loss: 0.6923 - val_accuracy: 0.6467 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6923 - accuracy: 0.5252 - val_loss: 0.6921 - val_accuracy: 0.5484 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6922 - accuracy: 0.5259 - val_loss: 0.6920 - val_accuracy: 0.5627 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6920 - accuracy: 0.5440 - val_loss: 0.6918 - val_accuracy: 0.5691 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6919 - accuracy: 0.5360 - val_loss: 0.6920 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6916 - accuracy: 0.5421 - val_loss: 0.6917 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6913 - accuracy: 0.5474 - val_loss: 0.6911 - val_accuracy: 0.6507 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6910 - accuracy: 0.5903 - val_loss: 0.6913 - val_accuracy: 0.4908 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6906 - accuracy: 0.5515 - val_loss: 0.6903 - val_accuracy: 0.6603 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6901 - accuracy: 0.5982 - val_loss: 0.6896 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6893 - accuracy: 0.6144 - val_loss: 0.6887 - val_accuracy: 0.6715 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6882 - accuracy: 0.6303 - val_loss: 0.6874 - val_accuracy: 0.6715 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6866 - accuracy: 0.6635 - val_loss: 0.6858 - val_accuracy: 0.6371 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6840 - accuracy: 0.6620 - val_loss: 0.6818 - val_accuracy: 0.6922 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6788 - accuracy: 0.6835 - val_loss: 0.6741 - val_accuracy: 0.6986 - lr: 0.0100\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6655 - accuracy: 0.6986 - val_loss: 0.6496 - val_accuracy: 0.7178 - lr: 0.0100\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.5884 - accuracy: 0.7319 - val_loss: 0.4787 - val_accuracy: 0.7754 - lr: 0.0100\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.4642 - accuracy: 0.7819 - val_loss: 0.4487 - val_accuracy: 0.7978 - lr: 0.0100\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.4207 - accuracy: 0.8087 - val_loss: 0.6567 - val_accuracy: 0.6755 - lr: 0.0100\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3937 - accuracy: 0.8255 - val_loss: 0.3869 - val_accuracy: 0.8361 - lr: 0.0100\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3769 - accuracy: 0.8372 - val_loss: 0.4248 - val_accuracy: 0.7954 - lr: 0.0100\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3620 - accuracy: 0.8412 - val_loss: 0.3848 - val_accuracy: 0.8177 - lr: 0.0100\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3498 - accuracy: 0.8458 - val_loss: 0.3162 - val_accuracy: 0.8673 - lr: 0.0100\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3367 - accuracy: 0.8570 - val_loss: 0.3620 - val_accuracy: 0.8457 - lr: 0.0100\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3267 - accuracy: 0.8610 - val_loss: 0.5597 - val_accuracy: 0.7370 - lr: 0.0100\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3212 - accuracy: 0.8640 - val_loss: 0.2907 - val_accuracy: 0.8817 - lr: 0.0100\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3110 - accuracy: 0.8668 - val_loss: 0.3029 - val_accuracy: 0.8593 - lr: 0.0100\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.3073 - accuracy: 0.8701 - val_loss: 0.4137 - val_accuracy: 0.8058 - lr: 0.0100\n",
      "Epoch 45/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2970 - accuracy: 0.8742 - val_loss: 0.2720 - val_accuracy: 0.8905 - lr: 0.0100\n",
      "Epoch 46/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2895 - accuracy: 0.8776 - val_loss: 0.2723 - val_accuracy: 0.8889 - lr: 0.0100\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.2832 - accuracy: 0.8807 - val_loss: 0.3094 - val_accuracy: 0.8569 - lr: 0.0100\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2821 - accuracy: 0.8804 - val_loss: 0.4198 - val_accuracy: 0.7994 - lr: 0.0100\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2732 - accuracy: 0.8861 - val_loss: 0.3277 - val_accuracy: 0.8577 - lr: 0.0100\n",
      "Epoch 50/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2651 - accuracy: 0.8893 - val_loss: 0.3902 - val_accuracy: 0.8401 - lr: 0.0100\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2629 - accuracy: 0.8913 - val_loss: 0.2572 - val_accuracy: 0.9017 - lr: 0.0100\n",
      "Epoch 52/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2529 - accuracy: 0.8946 - val_loss: 0.2392 - val_accuracy: 0.9033 - lr: 0.0100\n",
      "Epoch 53/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2486 - accuracy: 0.8964 - val_loss: 0.2472 - val_accuracy: 0.8945 - lr: 0.0100\n",
      "Epoch 54/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.2440 - accuracy: 0.8981 - val_loss: 0.4783 - val_accuracy: 0.7962 - lr: 0.0100\n",
      "Epoch 55/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2383 - accuracy: 0.9010 - val_loss: 0.2267 - val_accuracy: 0.9065 - lr: 0.0100\n",
      "Epoch 56/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2342 - accuracy: 0.9023 - val_loss: 0.3135 - val_accuracy: 0.8737 - lr: 0.0100\n",
      "Epoch 57/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2248 - accuracy: 0.9071 - val_loss: 0.2256 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 58/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2256 - accuracy: 0.9090 - val_loss: 0.2347 - val_accuracy: 0.9065 - lr: 0.0100\n",
      "Epoch 59/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2170 - accuracy: 0.9112 - val_loss: 0.3276 - val_accuracy: 0.8585 - lr: 0.0100\n",
      "Epoch 60/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2076 - accuracy: 0.9155 - val_loss: 0.2263 - val_accuracy: 0.9041 - lr: 0.0100\n",
      "Epoch 61/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2078 - accuracy: 0.9160 - val_loss: 0.2857 - val_accuracy: 0.8833 - lr: 0.0100\n",
      "Epoch 62/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1968 - accuracy: 0.9190 - val_loss: 0.2285 - val_accuracy: 0.9081 - lr: 0.0100\n",
      "Epoch 63/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1980 - accuracy: 0.9193 - val_loss: 0.3728 - val_accuracy: 0.8569 - lr: 0.0100\n",
      "Epoch 64/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9249\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1876 - accuracy: 0.9249 - val_loss: 0.2633 - val_accuracy: 0.8921 - lr: 0.0100\n",
      "Epoch 65/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1409 - accuracy: 0.9467 - val_loss: 0.2123 - val_accuracy: 0.9161 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1332 - accuracy: 0.9493 - val_loss: 0.2164 - val_accuracy: 0.9161 - lr: 1.0000e-03\n",
      "Epoch 67/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1301 - accuracy: 0.9505 - val_loss: 0.2180 - val_accuracy: 0.9153 - lr: 1.0000e-03\n",
      "Epoch 68/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1276 - accuracy: 0.9507 - val_loss: 0.2275 - val_accuracy: 0.9153 - lr: 1.0000e-03\n",
      "Epoch 69/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1248 - accuracy: 0.9534 - val_loss: 0.2188 - val_accuracy: 0.9129 - lr: 1.0000e-03\n",
      "Epoch 70/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1225 - accuracy: 0.9542 - val_loss: 0.2189 - val_accuracy: 0.9161 - lr: 1.0000e-03\n",
      "Epoch 71/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1211 - accuracy: 0.9546 - val_loss: 0.2320 - val_accuracy: 0.9153 - lr: 1.0000e-03\n",
      "Epoch 72/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9547\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1184 - accuracy: 0.9547 - val_loss: 0.2229 - val_accuracy: 0.9169 - lr: 1.0000e-03\n",
      "Epoch 73/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1133 - accuracy: 0.9586 - val_loss: 0.2227 - val_accuracy: 0.9161 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1128 - accuracy: 0.9585 - val_loss: 0.2220 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1126 - accuracy: 0.9588 - val_loss: 0.2221 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.1122 - accuracy: 0.9587 - val_loss: 0.2222 - val_accuracy: 0.9169 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1119 - accuracy: 0.9587 - val_loss: 0.2222 - val_accuracy: 0.9161 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.1117 - accuracy: 0.9592 - val_loss: 0.2229 - val_accuracy: 0.9161 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "741/743 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9591\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1114 - accuracy: 0.9591 - val_loss: 0.2233 - val_accuracy: 0.9177 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1108 - accuracy: 0.9593 - val_loss: 0.2235 - val_accuracy: 0.9177 - lr: 1.0000e-05\n",
      "Epoch 81/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1108 - accuracy: 0.9593 - val_loss: 0.2234 - val_accuracy: 0.9177 - lr: 1.0000e-05\n",
      "Epoch 82/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1108 - accuracy: 0.9591 - val_loss: 0.2236 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Epoch 83/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1107 - accuracy: 0.9592 - val_loss: 0.2236 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Epoch 84/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1107 - accuracy: 0.9592 - val_loss: 0.2237 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Epoch 85/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1107 - accuracy: 0.9592 - val_loss: 0.2237 - val_accuracy: 0.9169 - lr: 1.0000e-05\n",
      "Score fold 1: loss de 0.254878431558609; accuracy de 90.70297479629517%\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6932 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.6930 - accuracy: 0.5055 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 7s 10ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6938 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6935 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5078 - val_loss: 0.6935 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6933 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5082 - val_loss: 0.6931 - val_accuracy: 0.5739 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5084\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5084 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5080\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-05\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-05\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-05\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-05\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5080 - val_loss: 0.6934 - val_accuracy: 0.4924 - lr: 1.0000e-05\n",
      "Score fold 2: loss de 0.6930822730064392; accuracy de 58.224356174468994%\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5041 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5039 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5045 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6931 - accuracy: 0.5042 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.5037\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5037 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5048\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.6930 - accuracy: 0.5048\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 10ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5048 - val_loss: 0.6930 - val_accuracy: 0.5076 - lr: 1.0000e-05\n",
      "Score fold 3: loss de 0.6930049657821655; accuracy de 50.64187049865723%\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 4.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 10s 11ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6931 - val_accuracy: 0.5292 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5052 - val_loss: 0.6933 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5058 - val_loss: 0.6931 - val_accuracy: 0.5236 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6937 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6937 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6931 - accuracy: 0.5063 - val_loss: 0.6933 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6930 - accuracy: 0.5064 - val_loss: 0.6937 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.5077\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6930 - accuracy: 0.5077 - val_loss: 0.6935 - val_accuracy: 0.4876 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6935 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6935 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "741/743 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5063\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 22/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.6929 - accuracy: 0.5063\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6929 - accuracy: 0.5063 - val_loss: 0.6934 - val_accuracy: 0.4876 - lr: 1.0000e-05\n",
      "Score fold 4: loss de 0.6930965185165405; accuracy de 56.29274249076843%\n"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(gpt2Data, target):\n",
    "    model = keras.models.Sequential([\n",
    "        # keras.layers.LSTM(50, activation='tanh',recurrent_activation='sigmoid',input_shape=[1, gpt2Data.shape[2]]),\n",
    "\n",
    "        ########## Stacked LSTM\n",
    "        keras.layers.LSTM(784, activation='relu', return_sequences=True, input_shape=[1, gpt2Data.shape[2]]),\n",
    "        keras.layers.LSTM(250, activation='relu', return_sequences=True),\n",
    "        keras.layers.LSTM(90, activation='relu', return_sequences=True),\n",
    "        keras.layers.LSTM(30, activation='relu'),\n",
    "        keras.layers.Dense(len(set(target)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(gpt2Data[train], target[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(gpt2Data[test], target[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Score de cada fold:\n",
      "****************************************************\n",
      "--> Fold 1: Loss: 0.254878431558609 ; Accuracy: 90.70297479629517%\n",
      "****************************************************\n",
      "--> Fold 2: Loss: 0.6930822730064392 ; Accuracy: 58.224356174468994%\n",
      "****************************************************\n",
      "--> Fold 3: Loss: 0.6930049657821655 ; Accuracy: 50.64187049865723%\n",
      "****************************************************\n",
      "--> Fold 4: Loss: 0.6930965185165405 ; Accuracy: 56.29274249076843%\n",
      "****************************************************\n",
      "Média de accuracy das folds:\n",
      "--> Accuracy: 63.965485990047455 (+- 15.686328456586708)\n",
      "--> Loss: 0.5835155472159386\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#categories = [\"Ham\", \"Spam\"]\n",
    "#\n",
    "#skplt.metrics.plot_confusion_matrix(\n",
    "#    [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#    title=\"Confusion Matrix\",\n",
    "#    cmap=\"Purples\",\n",
    "#    hide_zeros=True,\n",
    "#    figsize=(5,5)\n",
    "#)\n",
    "#\n",
    "#plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# skplt.metrics.plot_confusion_matrix(\n",
    "#     [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#     normalize=True,\n",
    "#     title=\"Confusion Matrix\",\n",
    "#     cmap=\"Purples\",\n",
    "#     hide_zeros=True,\n",
    "#     figsize=(5,5)\n",
    "# )\n",
    "#\n",
    "# plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}