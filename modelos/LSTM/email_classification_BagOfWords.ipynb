{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Carregando base de dados  pré-processada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   email  target\n0      start date     hourahead timee  cardinall  hou...       0\n1      service long desk  price structure deal quote ...       0\n2      start date  cardinall    hourahead timee  card...       0\n3      start date     hourahead timee  cardinall  anc...       0\n4      cardinall deliverable revenue management marke...       0\n...                                                  ...     ...\n33340  bio  matrix scientific group   symbo   bmxg  p...       1\n33341   cardinall step away hot naked webcam girl liv...       1\n33342  need pill increase performance click  seroius ...       1\n33343  datee final nom       inlet hpl  eastrans  car...       0\n33344  ordinall time  offering male enhancement perfo...       1\n\n[33341 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>start date     hourahead timee  cardinall  hou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>service long desk  price structure deal quote ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>start date  cardinall    hourahead timee  card...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>start date     hourahead timee  cardinall  anc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cardinall deliverable revenue management marke...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33341</th>\n      <td>cardinall step away hot naked webcam girl liv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33342</th>\n      <td>need pill increase performance click  seroius ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33343</th>\n      <td>datee final nom       inlet hpl  eastrans  car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33344</th>\n      <td>ordinall time  offering male enhancement perfo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "target = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Representação vetorial Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       aa  ability  able  absolutely  abuse  accept  acceptance  accepted  \\\n0       0        0     0           0      0       0           0         0   \n1       0        0     0           0      0       0           0         0   \n2       0        0     0           0      0       0           0         0   \n3       0        0     0           0      0       0           0         0   \n4       0        0     0           0      0       0           0         0   \n...    ..      ...   ...         ...    ...     ...         ...       ...   \n33336   0        0     0           0      0       0           1         0   \n33337   0        0     0           0      0       0           0         0   \n33338   0        0     0           0      0       0           0         0   \n33339   0        0     0           0      0       0           0         0   \n33340   0        0     0           0      0       0           0         0   \n\n       access  according  ...  xanax  xl  xp  yahoo  year  yes  yield  yo  \\\n0           0          0  ...      0   0   0      0     0    0      0   0   \n1           0          0  ...      0   0   0      0     0    0      0   0   \n2           0          0  ...      0   0   0      0     0    0      0   0   \n3           0          0  ...      0   0   0      0     0    0      0   0   \n4           1          0  ...      0   0   0      0     0    0      0   0   \n...       ...        ...  ...    ...  ..  ..    ...   ...  ...    ...  ..   \n33336       0          0  ...      0   0   0      1     0    0      0   0   \n33337       1          0  ...      0   0   0      0     0    0      0   0   \n33338       0          0  ...      0   0   0      0     0    0      0   0   \n33339       0          0  ...      0   0   0      0     0    0      0   0   \n33340       0          0  ...      0   0   0      0     0    0      0   0   \n\n       young  zone  \n0          0     0  \n1          0     0  \n2          0     0  \n3          0     0  \n4          0     0  \n...      ...   ...  \n33336      0     0  \n33337      0     0  \n33338      0     0  \n33339      0     0  \n33340      0     0  \n\n[33341 rows x 2100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>absolutely</th>\n      <th>abuse</th>\n      <th>accept</th>\n      <th>acceptance</th>\n      <th>accepted</th>\n      <th>access</th>\n      <th>according</th>\n      <th>...</th>\n      <th>xanax</th>\n      <th>xl</th>\n      <th>xp</th>\n      <th>yahoo</th>\n      <th>year</th>\n      <th>yes</th>\n      <th>yield</th>\n      <th>yo</th>\n      <th>young</th>\n      <th>zone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33336</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33337</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33338</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33339</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2100 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "\n",
    "bag = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "del emailsText\n",
    "del X\n",
    "\n",
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = np.array(bag)\n",
    "bag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualização de dados com TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = TSNE(n_components=2, random_state=0)\n",
    "# #model = PCA(n_components=50, svd_solver='full')\n",
    "# array_red = model.fit_transform(bag)\n",
    "#\n",
    "# df_tsne = pd.DataFrame(array_red)\n",
    "#\n",
    "# df_tsne['Target'] = target\n",
    "# df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "#\n",
    "# df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "#\n",
    "# plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "#\n",
    "# plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "#\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "#\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33341, 1, 2100)\n"
     ]
    }
   ],
   "source": [
    "bag = bag.reshape((bag.shape[0], 1, bag.shape[1]))\n",
    "\n",
    "print(bag.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 12s 12ms/step - loss: 0.6928 - accuracy: 0.5094 - val_loss: 0.6929 - val_accuracy: 0.5076 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.6920 - accuracy: 0.5148 - val_loss: 0.6937 - val_accuracy: 0.4868 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6913 - accuracy: 0.5112 - val_loss: 0.6926 - val_accuracy: 0.4884 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6908 - accuracy: 0.5147 - val_loss: 0.6914 - val_accuracy: 0.5827 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6904 - accuracy: 0.5280 - val_loss: 0.6916 - val_accuracy: 0.4916 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6900 - accuracy: 0.5198 - val_loss: 0.6910 - val_accuracy: 0.4940 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6896 - accuracy: 0.5303 - val_loss: 0.6909 - val_accuracy: 0.4948 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6888 - accuracy: 0.5331 - val_loss: 0.6899 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6878 - accuracy: 0.5410 - val_loss: 0.6889 - val_accuracy: 0.5116 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6861 - accuracy: 0.5565 - val_loss: 0.6870 - val_accuracy: 0.5228 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6827 - accuracy: 0.5634 - val_loss: 0.6819 - val_accuracy: 0.5548 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6759 - accuracy: 0.5884 - val_loss: 0.6748 - val_accuracy: 0.5691 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6647 - accuracy: 0.6038 - val_loss: 0.6602 - val_accuracy: 0.6003 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6439 - accuracy: 0.6303 - val_loss: 0.6341 - val_accuracy: 0.6235 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6092 - accuracy: 0.6642 - val_loss: 0.5921 - val_accuracy: 0.6611 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.5671 - accuracy: 0.7017 - val_loss: 0.5781 - val_accuracy: 0.6787 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.5055 - accuracy: 0.7511 - val_loss: 0.4678 - val_accuracy: 0.7602 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.4220 - accuracy: 0.8071 - val_loss: 0.3906 - val_accuracy: 0.8321 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3458 - accuracy: 0.8547 - val_loss: 0.3173 - val_accuracy: 0.8657 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2814 - accuracy: 0.8891 - val_loss: 0.2574 - val_accuracy: 0.8937 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2268 - accuracy: 0.9174 - val_loss: 0.2107 - val_accuracy: 0.9281 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1894 - accuracy: 0.9323 - val_loss: 0.1766 - val_accuracy: 0.9408 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1600 - accuracy: 0.9435 - val_loss: 0.1610 - val_accuracy: 0.9528 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.1400 - accuracy: 0.9515 - val_loss: 0.1620 - val_accuracy: 0.9568 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1280 - accuracy: 0.9560 - val_loss: 0.1851 - val_accuracy: 0.9592 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1111 - accuracy: 0.9622 - val_loss: 0.1958 - val_accuracy: 0.9624 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1003 - accuracy: 0.9668 - val_loss: 0.2358 - val_accuracy: 0.9648 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1010 - accuracy: 0.9665 - val_loss: 0.2426 - val_accuracy: 0.9640 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0866 - accuracy: 0.9708 - val_loss: 0.2525 - val_accuracy: 0.9640 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.9740\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0780 - accuracy: 0.9741 - val_loss: 0.2733 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0734 - accuracy: 0.9760 - val_loss: 0.2734 - val_accuracy: 0.9688 - lr: 1.0000e-03\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0725 - accuracy: 0.9762 - val_loss: 0.2765 - val_accuracy: 0.9680 - lr: 1.0000e-03\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0721 - accuracy: 0.9761 - val_loss: 0.2779 - val_accuracy: 0.9688 - lr: 1.0000e-03\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0713 - accuracy: 0.9763 - val_loss: 0.2800 - val_accuracy: 0.9680 - lr: 1.0000e-03\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0706 - accuracy: 0.9767 - val_loss: 0.2824 - val_accuracy: 0.9688 - lr: 1.0000e-03\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0700 - accuracy: 0.9769 - val_loss: 0.2838 - val_accuracy: 0.9688 - lr: 1.0000e-03\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9773\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0695 - accuracy: 0.9773 - val_loss: 0.2865 - val_accuracy: 0.9680 - lr: 1.0000e-03\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0690 - accuracy: 0.9775 - val_loss: 0.2861 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0689 - accuracy: 0.9776 - val_loss: 0.2861 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0689 - accuracy: 0.9776 - val_loss: 0.2863 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0688 - accuracy: 0.9776 - val_loss: 0.2865 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0688 - accuracy: 0.9775 - val_loss: 0.2868 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0687 - accuracy: 0.9775 - val_loss: 0.2870 - val_accuracy: 0.9688 - lr: 1.0000e-04\n",
      "Score fold 1: loss de 0.15225523710250854; accuracy de 94.93762254714966%\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 11s 12ms/step - loss: 0.6928 - accuracy: 0.5063 - val_loss: 0.6928 - val_accuracy: 0.4964 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6919 - accuracy: 0.5111 - val_loss: 0.6919 - val_accuracy: 0.5060 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6914 - accuracy: 0.5157 - val_loss: 0.6920 - val_accuracy: 0.4972 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6910 - accuracy: 0.5123 - val_loss: 0.6915 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6906 - accuracy: 0.5151 - val_loss: 0.6915 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6904 - accuracy: 0.5140 - val_loss: 0.6910 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6900 - accuracy: 0.5149 - val_loss: 0.6903 - val_accuracy: 0.5180 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6896 - accuracy: 0.5210 - val_loss: 0.6899 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6889 - accuracy: 0.5273 - val_loss: 0.6895 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6881 - accuracy: 0.5263 - val_loss: 0.6883 - val_accuracy: 0.5212 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6867 - accuracy: 0.5357 - val_loss: 0.6866 - val_accuracy: 0.5548 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6847 - accuracy: 0.5551 - val_loss: 0.6841 - val_accuracy: 0.5564 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6808 - accuracy: 0.5634 - val_loss: 0.6787 - val_accuracy: 0.5779 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6741 - accuracy: 0.5780 - val_loss: 0.6689 - val_accuracy: 0.6019 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6633 - accuracy: 0.6064 - val_loss: 0.6550 - val_accuracy: 0.5987 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6459 - accuracy: 0.6207 - val_loss: 0.6325 - val_accuracy: 0.6323 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6165 - accuracy: 0.6553 - val_loss: 0.5954 - val_accuracy: 0.6659 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.5688 - accuracy: 0.6968 - val_loss: 0.5382 - val_accuracy: 0.7122 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.5057 - accuracy: 0.7469 - val_loss: 0.4640 - val_accuracy: 0.7674 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.4342 - accuracy: 0.7963 - val_loss: 0.3790 - val_accuracy: 0.8321 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3602 - accuracy: 0.8463 - val_loss: 0.2993 - val_accuracy: 0.8753 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2932 - accuracy: 0.8831 - val_loss: 0.2378 - val_accuracy: 0.9137 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2362 - accuracy: 0.9128 - val_loss: 0.1860 - val_accuracy: 0.9416 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1949 - accuracy: 0.9320 - val_loss: 0.1534 - val_accuracy: 0.9512 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1639 - accuracy: 0.9436 - val_loss: 0.1318 - val_accuracy: 0.9592 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1440 - accuracy: 0.9507 - val_loss: 0.1147 - val_accuracy: 0.9616 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.1263 - accuracy: 0.9574 - val_loss: 0.1273 - val_accuracy: 0.9600 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1124 - accuracy: 0.9623 - val_loss: 0.1781 - val_accuracy: 0.9664 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1121 - accuracy: 0.9643 - val_loss: 0.2942 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1056 - accuracy: 0.9669 - val_loss: 0.0954 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1031 - accuracy: 0.9662 - val_loss: 0.0812 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1810 - accuracy: 0.9435 - val_loss: 0.0966 - val_accuracy: 0.9656 - lr: 0.0100\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0957 - accuracy: 0.9683 - val_loss: 0.0877 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0865 - accuracy: 0.9714 - val_loss: 0.0872 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0773 - accuracy: 0.9738 - val_loss: 0.1113 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0726 - accuracy: 0.9755 - val_loss: 0.1205 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0747 - accuracy: 0.9757 - val_loss: 0.0828 - val_accuracy: 0.9704 - lr: 0.0100\n",
      "Epoch 38/200\n",
      "738/743 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9671\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1123 - accuracy: 0.9671 - val_loss: 0.1080 - val_accuracy: 0.9704 - lr: 0.0100\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0715 - accuracy: 0.9763 - val_loss: 0.1162 - val_accuracy: 0.9720 - lr: 1.0000e-03\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0685 - accuracy: 0.9770 - val_loss: 0.1192 - val_accuracy: 0.9720 - lr: 1.0000e-03\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0661 - accuracy: 0.9779 - val_loss: 0.1151 - val_accuracy: 0.9720 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0652 - accuracy: 0.9784 - val_loss: 0.1143 - val_accuracy: 0.9728 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0642 - accuracy: 0.9790 - val_loss: 0.1135 - val_accuracy: 0.9712 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0632 - accuracy: 0.9788 - val_loss: 0.1139 - val_accuracy: 0.9712 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "741/743 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9796\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0627 - accuracy: 0.9796 - val_loss: 0.1138 - val_accuracy: 0.9720 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0620 - accuracy: 0.9799 - val_loss: 0.1139 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0619 - accuracy: 0.9799 - val_loss: 0.1137 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0618 - accuracy: 0.9798 - val_loss: 0.1138 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0618 - accuracy: 0.9798 - val_loss: 0.1139 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Epoch 50/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0617 - accuracy: 0.9800 - val_loss: 0.1139 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0616 - accuracy: 0.9798 - val_loss: 0.1140 - val_accuracy: 0.9720 - lr: 1.0000e-04\n",
      "Score fold 2: loss de 0.10669742524623871; accuracy de 96.2207555770874%\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 11s 12ms/step - loss: 0.6928 - accuracy: 0.5078 - val_loss: 0.6925 - val_accuracy: 0.5060 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6920 - accuracy: 0.5112 - val_loss: 0.6916 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6914 - accuracy: 0.5134 - val_loss: 0.6910 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6909 - accuracy: 0.5140 - val_loss: 0.6907 - val_accuracy: 0.5052 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6906 - accuracy: 0.5190 - val_loss: 0.6906 - val_accuracy: 0.5060 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6903 - accuracy: 0.5150 - val_loss: 0.6900 - val_accuracy: 0.5156 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6899 - accuracy: 0.5334 - val_loss: 0.6896 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6894 - accuracy: 0.5284 - val_loss: 0.6890 - val_accuracy: 0.5188 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6887 - accuracy: 0.5445 - val_loss: 0.6884 - val_accuracy: 0.5116 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6877 - accuracy: 0.5414 - val_loss: 0.6872 - val_accuracy: 0.5372 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6860 - accuracy: 0.5545 - val_loss: 0.6853 - val_accuracy: 0.5604 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6829 - accuracy: 0.5679 - val_loss: 0.6822 - val_accuracy: 0.5739 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6773 - accuracy: 0.5838 - val_loss: 0.6772 - val_accuracy: 0.5763 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6682 - accuracy: 0.5925 - val_loss: 0.6671 - val_accuracy: 0.6051 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6524 - accuracy: 0.6181 - val_loss: 0.6484 - val_accuracy: 0.6235 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6244 - accuracy: 0.6447 - val_loss: 0.6105 - val_accuracy: 0.6603 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.5796 - accuracy: 0.6902 - val_loss: 0.5562 - val_accuracy: 0.6978 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.5214 - accuracy: 0.7347 - val_loss: 0.4991 - val_accuracy: 0.7458 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.4544 - accuracy: 0.7832 - val_loss: 0.4272 - val_accuracy: 0.7994 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 9s 13ms/step - loss: 0.3850 - accuracy: 0.8309 - val_loss: 0.3718 - val_accuracy: 0.8321 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3189 - accuracy: 0.8703 - val_loss: 0.2936 - val_accuracy: 0.8777 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2680 - accuracy: 0.8945 - val_loss: 0.2481 - val_accuracy: 0.8993 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2214 - accuracy: 0.9199 - val_loss: 0.2074 - val_accuracy: 0.9201 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1864 - accuracy: 0.9343 - val_loss: 0.1749 - val_accuracy: 0.9384 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3568 - accuracy: 0.8578 - val_loss: 0.2161 - val_accuracy: 0.9153 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2012 - accuracy: 0.9303 - val_loss: 0.1614 - val_accuracy: 0.9496 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.2237 - accuracy: 0.9284 - val_loss: 0.1616 - val_accuracy: 0.9376 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1497 - accuracy: 0.9497 - val_loss: 0.1321 - val_accuracy: 0.9552 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.1203 - accuracy: 0.9604 - val_loss: 0.1102 - val_accuracy: 0.9584 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1036 - accuracy: 0.9653 - val_loss: 0.0982 - val_accuracy: 0.9624 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0950 - accuracy: 0.9693 - val_loss: 0.0943 - val_accuracy: 0.9616 - lr: 0.0100\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0869 - accuracy: 0.9713 - val_loss: 0.0872 - val_accuracy: 0.9664 - lr: 0.0100\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0782 - accuracy: 0.9736 - val_loss: 0.0780 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0750 - accuracy: 0.9748 - val_loss: 0.0748 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0733 - accuracy: 0.9761 - val_loss: 0.0746 - val_accuracy: 0.9688 - lr: 0.0100\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1309 - accuracy: 0.9606 - val_loss: 0.1745 - val_accuracy: 0.9520 - lr: 0.0100\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0771 - accuracy: 0.9747 - val_loss: 0.0668 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0652 - accuracy: 0.9788 - val_loss: 0.0820 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0578 - accuracy: 0.9812 - val_loss: 0.0623 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0639 - accuracy: 0.9796 - val_loss: 0.0834 - val_accuracy: 0.9688 - lr: 0.0100\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0557 - accuracy: 0.9822 - val_loss: 0.0597 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0560 - accuracy: 0.9826 - val_loss: 0.0609 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0606 - accuracy: 0.9812 - val_loss: 0.0709 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0577 - accuracy: 0.9811 - val_loss: 0.0649 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 45/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.0464 - accuracy: 0.9852 - val_loss: 0.0657 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 46/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0431 - accuracy: 0.9867 - val_loss: 0.0558 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.0555 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0555 - accuracy: 0.9846 - val_loss: 0.0664 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.0409 - accuracy: 0.9875 - val_loss: 0.0527 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 50/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0970 - accuracy: 0.9755 - val_loss: 0.5496 - val_accuracy: 0.8473 - lr: 0.0100\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0847 - accuracy: 0.9716 - val_loss: 0.0760 - val_accuracy: 0.9696 - lr: 0.0100\n",
      "Epoch 52/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0574 - accuracy: 0.9807 - val_loss: 0.0579 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 53/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0461 - accuracy: 0.9856 - val_loss: 0.0651 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 54/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0422 - accuracy: 0.9869 - val_loss: 0.0530 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 55/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0394 - accuracy: 0.9883 - val_loss: 0.0661 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 56/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.0380 - accuracy: 0.9888\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0383 - accuracy: 0.9886 - val_loss: 0.0878 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 57/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0344 - accuracy: 0.9904 - val_loss: 0.0561 - val_accuracy: 0.9784 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.0541 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0333 - accuracy: 0.9909 - val_loss: 0.0551 - val_accuracy: 0.9784 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0333 - accuracy: 0.9907 - val_loss: 0.0530 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0329 - accuracy: 0.9909 - val_loss: 0.0539 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0327 - accuracy: 0.9910 - val_loss: 0.0541 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "740/743 [============================>.] - ETA: 0s - loss: 0.0325 - accuracy: 0.9910\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0326 - accuracy: 0.9909 - val_loss: 0.0537 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0322 - accuracy: 0.9912 - val_loss: 0.0538 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0322 - accuracy: 0.9912 - val_loss: 0.0540 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "743/743 [==============================] - 9s 13ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 0.0542 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 0.0556 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0318 - accuracy: 0.9912 - val_loss: 0.0545 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "743/743 [==============================] - 10s 13ms/step - loss: 0.0317 - accuracy: 0.9913 - val_loss: 0.0545 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Score fold 3: loss de 0.07980623096227646; accuracy de 97.6604700088501%\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 4.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 11s 12ms/step - loss: 0.6930 - accuracy: 0.5067 - val_loss: 0.6932 - val_accuracy: 0.4980 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6928 - accuracy: 0.5103 - val_loss: 0.6930 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6926 - accuracy: 0.5103 - val_loss: 0.6926 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6920 - accuracy: 0.5154 - val_loss: 0.6922 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6914 - accuracy: 0.5116 - val_loss: 0.6915 - val_accuracy: 0.5012 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 9s 11ms/step - loss: 0.6910 - accuracy: 0.5149 - val_loss: 0.6914 - val_accuracy: 0.5004 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6906 - accuracy: 0.5130 - val_loss: 0.6911 - val_accuracy: 0.5084 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6903 - accuracy: 0.5163 - val_loss: 0.6910 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6899 - accuracy: 0.5186 - val_loss: 0.6908 - val_accuracy: 0.5052 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6895 - accuracy: 0.5224 - val_loss: 0.6908 - val_accuracy: 0.5036 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6890 - accuracy: 0.5242 - val_loss: 0.6901 - val_accuracy: 0.5092 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6883 - accuracy: 0.5340 - val_loss: 0.6895 - val_accuracy: 0.5132 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6872 - accuracy: 0.5393 - val_loss: 0.6881 - val_accuracy: 0.5212 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6851 - accuracy: 0.5648 - val_loss: 0.6864 - val_accuracy: 0.5204 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.6814 - accuracy: 0.5614 - val_loss: 0.6817 - val_accuracy: 0.5612 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6745 - accuracy: 0.5819 - val_loss: 0.6735 - val_accuracy: 0.5907 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6623 - accuracy: 0.6026 - val_loss: 0.6585 - val_accuracy: 0.6155 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.6399 - accuracy: 0.6340 - val_loss: 0.6273 - val_accuracy: 0.6459 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.5987 - accuracy: 0.6741 - val_loss: 0.5752 - val_accuracy: 0.6954 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.5352 - accuracy: 0.7258 - val_loss: 0.5055 - val_accuracy: 0.7386 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.4697 - accuracy: 0.7731 - val_loss: 0.4344 - val_accuracy: 0.7906 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.3996 - accuracy: 0.8197 - val_loss: 0.3607 - val_accuracy: 0.8449 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.3298 - accuracy: 0.8637 - val_loss: 0.3066 - val_accuracy: 0.8641 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2763 - accuracy: 0.8881 - val_loss: 0.2514 - val_accuracy: 0.9001 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.2299 - accuracy: 0.9153 - val_loss: 0.2081 - val_accuracy: 0.9233 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1958 - accuracy: 0.9292 - val_loss: 0.1857 - val_accuracy: 0.9297 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1677 - accuracy: 0.9400 - val_loss: 0.2050 - val_accuracy: 0.9209 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1514 - accuracy: 0.9470 - val_loss: 0.1486 - val_accuracy: 0.9424 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1280 - accuracy: 0.9556 - val_loss: 0.1322 - val_accuracy: 0.9496 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1150 - accuracy: 0.9606 - val_loss: 0.2314 - val_accuracy: 0.9265 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1067 - accuracy: 0.9641 - val_loss: 0.1309 - val_accuracy: 0.9568 - lr: 0.0100\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0927 - accuracy: 0.9696 - val_loss: 0.1290 - val_accuracy: 0.9616 - lr: 0.0100\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1163 - accuracy: 0.9683 - val_loss: 0.4925 - val_accuracy: 0.8585 - lr: 0.0100\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1426 - accuracy: 0.9555 - val_loss: 0.1431 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0842 - accuracy: 0.9730 - val_loss: 0.1241 - val_accuracy: 0.9696 - lr: 0.0100\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0720 - accuracy: 0.9765 - val_loss: 0.1314 - val_accuracy: 0.9696 - lr: 0.0100\n",
      "Epoch 37/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0963 - accuracy: 0.9701 - val_loss: 0.1451 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 38/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0758 - accuracy: 0.9765 - val_loss: 0.1547 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 39/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.1037 - accuracy: 0.9644 - val_loss: 0.1135 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 40/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0634 - accuracy: 0.9797 - val_loss: 0.1224 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 41/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0571 - accuracy: 0.9819 - val_loss: 0.1592 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 42/200\n",
      "743/743 [==============================] - 9s 12ms/step - loss: 0.0532 - accuracy: 0.9834 - val_loss: 0.1551 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 43/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0896 - accuracy: 0.9736 - val_loss: 0.2913 - val_accuracy: 0.9305 - lr: 0.0100\n",
      "Epoch 44/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.1010 - accuracy: 0.9605 - val_loss: 0.1641 - val_accuracy: 0.9680 - lr: 0.0100\n",
      "Epoch 45/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0666 - accuracy: 0.9782 - val_loss: 0.1711 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 46/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.0567 - accuracy: 0.9814\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0567 - accuracy: 0.9814 - val_loss: 0.1666 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 47/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0521 - accuracy: 0.9834 - val_loss: 0.1631 - val_accuracy: 0.9744 - lr: 1.0000e-03\n",
      "Epoch 48/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.1619 - val_accuracy: 0.9744 - lr: 1.0000e-03\n",
      "Epoch 49/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0506 - accuracy: 0.9839 - val_loss: 0.1617 - val_accuracy: 0.9752 - lr: 1.0000e-03\n",
      "Epoch 50/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0502 - accuracy: 0.9840 - val_loss: 0.1619 - val_accuracy: 0.9752 - lr: 1.0000e-03\n",
      "Epoch 51/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0497 - accuracy: 0.9842 - val_loss: 0.1618 - val_accuracy: 0.9752 - lr: 1.0000e-03\n",
      "Epoch 52/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0494 - accuracy: 0.9844 - val_loss: 0.1623 - val_accuracy: 0.9744 - lr: 1.0000e-03\n",
      "Epoch 53/200\n",
      "739/743 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9846\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0490 - accuracy: 0.9846 - val_loss: 0.1625 - val_accuracy: 0.9752 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0486 - accuracy: 0.9849 - val_loss: 0.1625 - val_accuracy: 0.9752 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 0.1625 - val_accuracy: 0.9752 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.1625 - val_accuracy: 0.9752 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.1626 - val_accuracy: 0.9752 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.1626 - val_accuracy: 0.9744 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "743/743 [==============================] - 8s 11ms/step - loss: 0.0484 - accuracy: 0.9848 - val_loss: 0.1626 - val_accuracy: 0.9744 - lr: 1.0000e-04\n",
      "Score fold 4: loss de 0.09429062157869339; accuracy de 97.15656638145447%\n"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(bag, target):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.LSTM(50, activation='tanh',recurrent_activation='sigmoid',input_shape=[1, bag.shape[2]]),\n",
    "\n",
    "        keras.layers.Dense(len(set(target)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(bag[train], target[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(bag[test], target[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Score de cada fold:\n",
      "****************************************************\n",
      "--> Fold 1: Loss: 0.15225523710250854 ; Accuracy: 94.93762254714966%\n",
      "****************************************************\n",
      "--> Fold 2: Loss: 0.10669742524623871 ; Accuracy: 96.2207555770874%\n",
      "****************************************************\n",
      "--> Fold 3: Loss: 0.07980623096227646 ; Accuracy: 97.6604700088501%\n",
      "****************************************************\n",
      "--> Fold 4: Loss: 0.09429062157869339 ; Accuracy: 97.15656638145447%\n",
      "****************************************************\n",
      "Média de accuracy das folds:\n",
      "--> Accuracy: 96.4938536286354 (+- 1.036414455716904)\n",
      "--> Loss: 0.10826237872242928\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#categories = [\"Ham\", \"Spam\"]\n",
    "#\n",
    "#skplt.metrics.plot_confusion_matrix(\n",
    "#    [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#    title=\"Confusion Matrix\",\n",
    "#    cmap=\"Purples\",\n",
    "#    hide_zeros=True,\n",
    "#    figsize=(5,5)\n",
    "#)\n",
    "#\n",
    "#plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# skplt.metrics.plot_confusion_matrix(\n",
    "#     [categories[i] for i in target], [categories[i] for i in predicoes.tolist()],\n",
    "#     normalize=True,\n",
    "#     title=\"Confusion Matrix\",\n",
    "#     cmap=\"Purples\",\n",
    "#     hide_zeros=True,\n",
    "#     figsize=(5,5)\n",
    "# )\n",
    "#\n",
    "# plt.xticks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}