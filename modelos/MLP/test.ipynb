{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   email  target\n0      start date     hourahead timee  cardinall  hou...       0\n1      service long desk  price structure deal quote ...       0\n2      start date  cardinall    hourahead timee  card...       0\n3      start date     hourahead timee  cardinall  anc...       0\n4      cardinall deliverable revenue management marke...       0\n...                                                  ...     ...\n33340  bio  matrix scientific group   symbo   bmxg  p...       1\n33341   cardinall step away hot naked webcam girl liv...       1\n33342  need pill increase performance click  seroius ...       1\n33343  datee final nom       inlet hpl  eastrans  car...       0\n33344  ordinall time  offering male enhancement perfo...       1\n\n[33341 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>start date     hourahead timee  cardinall  hou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>service long desk  price structure deal quote ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>start date  cardinall    hourahead timee  card...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>start date     hourahead timee  cardinall  anc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cardinall deliverable revenue management marke...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33341</th>\n      <td>cardinall step away hot naked webcam girl liv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33342</th>\n      <td>need pill increase performance click  seroius ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33343</th>\n      <td>datee final nom       inlet hpl  eastrans  car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33344</th>\n      <td>ordinall time  offering male enhancement perfo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "\n",
    "database = database.drop(columns=[\"Unnamed: 0\"])\n",
    "database = database.dropna()\n",
    "target = database[\"target\"].array\n",
    "database"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in database[\"email\"]:\n",
    "    emailsText.append(email)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       aa  ability  able  absolutely  abuse  accept  acceptance  accepted  \\\n0       0        0     0           0      0       0           0         0   \n1       0        0     0           0      0       0           0         0   \n2       0        0     0           0      0       0           0         0   \n3       0        0     0           0      0       0           0         0   \n4       0        0     0           0      0       0           0         0   \n...    ..      ...   ...         ...    ...     ...         ...       ...   \n33336   0        0     0           0      0       0           1         0   \n33337   0        0     0           0      0       0           0         0   \n33338   0        0     0           0      0       0           0         0   \n33339   0        0     0           0      0       0           0         0   \n33340   0        0     0           0      0       0           0         0   \n\n       access  according  ...  xanax  xl  xp  yahoo  year  yes  yield  yo  \\\n0           0          0  ...      0   0   0      0     0    0      0   0   \n1           0          0  ...      0   0   0      0     0    0      0   0   \n2           0          0  ...      0   0   0      0     0    0      0   0   \n3           0          0  ...      0   0   0      0     0    0      0   0   \n4           1          0  ...      0   0   0      0     0    0      0   0   \n...       ...        ...  ...    ...  ..  ..    ...   ...  ...    ...  ..   \n33336       0          0  ...      0   0   0      1     0    0      0   0   \n33337       1          0  ...      0   0   0      0     0    0      0   0   \n33338       0          0  ...      0   0   0      0     0    0      0   0   \n33339       0          0  ...      0   0   0      0     0    0      0   0   \n33340       0          0  ...      0   0   0      0     0    0      0   0   \n\n       young  zone  \n0          0     0  \n1          0     0  \n2          0     0  \n3          0     0  \n4          0     0  \n...      ...   ...  \n33336      0     0  \n33337      0     0  \n33338      0     0  \n33339      0     0  \n33340      0     0  \n\n[33341 rows x 2100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>absolutely</th>\n      <th>abuse</th>\n      <th>accept</th>\n      <th>acceptance</th>\n      <th>accepted</th>\n      <th>access</th>\n      <th>according</th>\n      <th>...</th>\n      <th>xanax</th>\n      <th>xl</th>\n      <th>xp</th>\n      <th>yahoo</th>\n      <th>year</th>\n      <th>yes</th>\n      <th>yield</th>\n      <th>yo</th>\n      <th>young</th>\n      <th>zone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33336</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33337</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33338</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33339</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows Ã— 2100 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=2100)\n",
    "XTrain = vectorizer.fit_transform(emailsText)\n",
    "\n",
    "bag = pd.DataFrame(XTrain.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "bag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(bag.values,target,test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_treino, X_teste), axis=0)\n",
    "targets = np.concatenate((y_treino, y_teste), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "acc_per_fold = []\n",
    "loss_per_fold = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 3s 2ms/step - loss: 0.4425 - accuracy: 0.8525 - val_loss: 0.2757 - val_accuracy: 0.8958 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2424 - accuracy: 0.9295 - val_loss: 0.1358 - val_accuracy: 0.9543 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1289 - accuracy: 0.9592 - val_loss: 0.1100 - val_accuracy: 0.9685 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9537 - val_loss: 0.1600 - val_accuracy: 0.9393 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2703 - accuracy: 0.9555 - val_loss: 0.1524 - val_accuracy: 0.9573 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1154 - accuracy: 0.9701 - val_loss: 0.1125 - val_accuracy: 0.9618 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0980 - accuracy: 0.9730 - val_loss: 0.1054 - val_accuracy: 0.9708 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1489 - accuracy: 0.9593 - val_loss: 0.0996 - val_accuracy: 0.9693 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0624 - accuracy: 0.9792 - val_loss: 0.0995 - val_accuracy: 0.9723 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 0.1005 - val_accuracy: 0.9753 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.1086 - val_accuracy: 0.9723 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0792 - accuracy: 0.9818 - val_loss: 0.1732 - val_accuracy: 0.9370 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0732 - accuracy: 0.9780 - val_loss: 0.1208 - val_accuracy: 0.9685 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0370 - accuracy: 0.9881 - val_loss: 0.1075 - val_accuracy: 0.9745 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0351 - accuracy: 0.9892 - val_loss: 0.1195 - val_accuracy: 0.9745 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "786/792 [============================>.] - ETA: 0s - loss: 0.0311 - accuracy: 0.9904\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0311 - accuracy: 0.9904 - val_loss: 0.1233 - val_accuracy: 0.9753 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.1183 - val_accuracy: 0.9768 - lr: 1.0000e-03\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0210 - accuracy: 0.9937 - val_loss: 0.1200 - val_accuracy: 0.9760 - lr: 1.0000e-03\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.1215 - val_accuracy: 0.9760 - lr: 1.0000e-03\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 0.1215 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 21/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 0.1205 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.1202 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9939\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.1222 - val_accuracy: 0.9745 - lr: 1.0000e-03\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0194 - accuracy: 0.9942 - val_loss: 0.1221 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1221 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1220 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1219 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.1220 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.1222 - val_accuracy: 0.9745 - lr: 1.0000e-04\n",
      "Score for fold 1: loss of 0.08282823115587234; accuracy of 97.27095365524292%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.3676 - accuracy: 0.8721 - val_loss: 0.2211 - val_accuracy: 0.9318 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2693 - accuracy: 0.9359 - val_loss: 0.1938 - val_accuracy: 0.9355 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1737 - accuracy: 0.9522 - val_loss: 0.1041 - val_accuracy: 0.9670 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1254 - accuracy: 0.9626 - val_loss: 0.0904 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0844 - accuracy: 0.9725 - val_loss: 0.2570 - val_accuracy: 0.9355 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0585 - accuracy: 0.9796 - val_loss: 0.0901 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0652 - accuracy: 0.9805 - val_loss: 0.0755 - val_accuracy: 0.9828 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0418 - accuracy: 0.9859 - val_loss: 0.0839 - val_accuracy: 0.9798 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0546 - accuracy: 0.9856 - val_loss: 0.0868 - val_accuracy: 0.9798 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0359 - accuracy: 0.9889 - val_loss: 0.0963 - val_accuracy: 0.9813 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0368 - accuracy: 0.9901 - val_loss: 0.0968 - val_accuracy: 0.9820 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0278 - accuracy: 0.9919 - val_loss: 0.0817 - val_accuracy: 0.9850 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0247 - accuracy: 0.9924 - val_loss: 0.1024 - val_accuracy: 0.9820 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "777/792 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9936\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0217 - accuracy: 0.9936 - val_loss: 0.0945 - val_accuracy: 0.9865 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.0972 - val_accuracy: 0.9850 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.0957 - val_accuracy: 0.9858 - lr: 1.0000e-03\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0985 - val_accuracy: 0.9843 - lr: 1.0000e-03\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.1002 - val_accuracy: 0.9850 - lr: 1.0000e-03\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.0977 - val_accuracy: 0.9858 - lr: 1.0000e-03\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0172 - accuracy: 0.9955 - val_loss: 0.0992 - val_accuracy: 0.9858 - lr: 1.0000e-03\n",
      "Epoch 21/200\n",
      "777/792 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9956\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0169 - accuracy: 0.9957 - val_loss: 0.1004 - val_accuracy: 0.9850 - lr: 1.0000e-03\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0999 - val_accuracy: 0.9858 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0998 - val_accuracy: 0.9858 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0996 - val_accuracy: 0.9858 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0995 - val_accuracy: 0.9865 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.0996 - val_accuracy: 0.9865 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.0996 - val_accuracy: 0.9858 - lr: 1.0000e-04\n",
      "Score for fold 2: loss of 0.09300064295530319; accuracy of 97.28554487228394%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.3763 - accuracy: 0.8731 - val_loss: 0.1719 - val_accuracy: 0.9430 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.9209 - val_loss: 0.1249 - val_accuracy: 0.9558 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1551 - accuracy: 0.9555 - val_loss: 0.1001 - val_accuracy: 0.9663 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1654 - accuracy: 0.9562 - val_loss: 0.1082 - val_accuracy: 0.9693 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0899 - accuracy: 0.9716 - val_loss: 0.1009 - val_accuracy: 0.9663 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0869 - accuracy: 0.9754 - val_loss: 0.0884 - val_accuracy: 0.9693 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0538 - accuracy: 0.9826 - val_loss: 0.0805 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0469 - accuracy: 0.9846 - val_loss: 0.0812 - val_accuracy: 0.9738 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0747 - accuracy: 0.9810 - val_loss: 0.0867 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1284 - accuracy: 0.9758 - val_loss: 0.0910 - val_accuracy: 0.9730 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.0809 - val_accuracy: 0.9745 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0521 - accuracy: 0.9848 - val_loss: 0.0905 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0530 - accuracy: 0.9839 - val_loss: 0.1097 - val_accuracy: 0.9685 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "778/792 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9853\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0644 - accuracy: 0.9853 - val_loss: 0.1155 - val_accuracy: 0.9670 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0474 - accuracy: 0.9868 - val_loss: 0.1022 - val_accuracy: 0.9693 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0435 - accuracy: 0.9877 - val_loss: 0.0992 - val_accuracy: 0.9723 - lr: 1.0000e-03\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0358 - accuracy: 0.9904 - val_loss: 0.0981 - val_accuracy: 0.9723 - lr: 1.0000e-03\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0330 - accuracy: 0.9912 - val_loss: 0.0959 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0309 - accuracy: 0.9917 - val_loss: 0.0954 - val_accuracy: 0.9745 - lr: 1.0000e-03\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0291 - accuracy: 0.9923 - val_loss: 0.0954 - val_accuracy: 0.9745 - lr: 1.0000e-03\n",
      "Epoch 21/200\n",
      "769/792 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9923\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0281 - accuracy: 0.9924 - val_loss: 0.0926 - val_accuracy: 0.9745 - lr: 1.0000e-03\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0271 - accuracy: 0.9929 - val_loss: 0.0937 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0269 - accuracy: 0.9929 - val_loss: 0.0938 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0268 - accuracy: 0.9929 - val_loss: 0.0936 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0267 - accuracy: 0.9930 - val_loss: 0.0937 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0266 - accuracy: 0.9930 - val_loss: 0.0937 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0939 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Score for fold 3: loss of 0.07573442161083221; accuracy of 97.51049876213074%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.3891 - accuracy: 0.8765 - val_loss: 0.1987 - val_accuracy: 0.9265 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2121 - accuracy: 0.9397 - val_loss: 0.1328 - val_accuracy: 0.9603 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1080 - accuracy: 0.9651 - val_loss: 0.0953 - val_accuracy: 0.9708 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.9686 - val_loss: 0.1008 - val_accuracy: 0.9685 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0685 - accuracy: 0.9770 - val_loss: 0.0908 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0541 - accuracy: 0.9829 - val_loss: 0.0765 - val_accuracy: 0.9805 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0537 - accuracy: 0.9843 - val_loss: 0.0850 - val_accuracy: 0.9790 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0464 - accuracy: 0.9864 - val_loss: 0.0851 - val_accuracy: 0.9783 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0378 - accuracy: 0.9884 - val_loss: 0.0913 - val_accuracy: 0.9813 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0334 - accuracy: 0.9903 - val_loss: 0.0944 - val_accuracy: 0.9798 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 0.0817 - val_accuracy: 0.9835 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0231 - accuracy: 0.9931 - val_loss: 0.0993 - val_accuracy: 0.9813 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "789/792 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9838\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0808 - accuracy: 0.9839 - val_loss: 0.0802 - val_accuracy: 0.9790 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0382 - accuracy: 0.9897 - val_loss: 0.0759 - val_accuracy: 0.9820 - lr: 1.0000e-03\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0332 - accuracy: 0.9913 - val_loss: 0.0753 - val_accuracy: 0.9805 - lr: 1.0000e-03\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0345 - accuracy: 0.9912 - val_loss: 0.0797 - val_accuracy: 0.9798 - lr: 1.0000e-03\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0284 - accuracy: 0.9927 - val_loss: 0.0799 - val_accuracy: 0.9813 - lr: 1.0000e-03\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0268 - accuracy: 0.9929 - val_loss: 0.0803 - val_accuracy: 0.9813 - lr: 1.0000e-03\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0256 - accuracy: 0.9933 - val_loss: 0.0824 - val_accuracy: 0.9813 - lr: 1.0000e-03\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0471 - accuracy: 0.9913 - val_loss: 0.0802 - val_accuracy: 0.9828 - lr: 1.0000e-03\n",
      "Epoch 21/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0247 - accuracy: 0.9938 - val_loss: 0.0790 - val_accuracy: 0.9820 - lr: 1.0000e-03\n",
      "Epoch 22/200\n",
      "788/792 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9921\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0366 - accuracy: 0.9921 - val_loss: 0.0758 - val_accuracy: 0.9813 - lr: 1.0000e-03\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0239 - accuracy: 0.9942 - val_loss: 0.0776 - val_accuracy: 0.9820 - lr: 1.0000e-04\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0234 - accuracy: 0.9941 - val_loss: 0.0797 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 0.0798 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0230 - accuracy: 0.9942 - val_loss: 0.0812 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0229 - accuracy: 0.9942 - val_loss: 0.0816 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 28/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0228 - accuracy: 0.9942 - val_loss: 0.0817 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 29/200\n",
      "786/792 [============================>.] - ETA: 0s - loss: 0.0227 - accuracy: 0.9943\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0227 - accuracy: 0.9943 - val_loss: 0.0818 - val_accuracy: 0.9828 - lr: 1.0000e-04\n",
      "Epoch 30/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.0818 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 31/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.0819 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 32/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0226 - accuracy: 0.9944 - val_loss: 0.0819 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 33/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.0820 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 34/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.0820 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Epoch 35/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.0820 - val_accuracy: 0.9828 - lr: 1.0000e-05\n",
      "Score for fold 4: loss of 0.07687965035438538; accuracy of 97.57048487663269%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.3592 - accuracy: 0.8689 - val_loss: 0.1739 - val_accuracy: 0.9423 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1638 - accuracy: 0.9525 - val_loss: 0.1210 - val_accuracy: 0.9625 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1075 - accuracy: 0.9661 - val_loss: 0.1554 - val_accuracy: 0.9483 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0914 - accuracy: 0.9703 - val_loss: 0.0920 - val_accuracy: 0.9723 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1540 - accuracy: 0.9579 - val_loss: 0.1783 - val_accuracy: 0.9460 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1566 - accuracy: 0.9599 - val_loss: 0.0979 - val_accuracy: 0.9670 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0888 - accuracy: 0.9743 - val_loss: 0.0836 - val_accuracy: 0.9715 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9547 - val_loss: 0.1394 - val_accuracy: 0.9505 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.2348 - accuracy: 0.9462 - val_loss: 0.1160 - val_accuracy: 0.9580 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0734 - accuracy: 0.9774 - val_loss: 0.0822 - val_accuracy: 0.9730 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0652 - accuracy: 0.9806 - val_loss: 0.0877 - val_accuracy: 0.9715 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0822 - accuracy: 0.9803 - val_loss: 0.0760 - val_accuracy: 0.9730 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0547 - accuracy: 0.9848 - val_loss: 0.0789 - val_accuracy: 0.9723 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0444 - accuracy: 0.9856 - val_loss: 0.0736 - val_accuracy: 0.9753 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0339 - accuracy: 0.9897 - val_loss: 0.0766 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0285 - accuracy: 0.9912 - val_loss: 0.0899 - val_accuracy: 0.9715 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0782 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0220 - accuracy: 0.9936 - val_loss: 0.0843 - val_accuracy: 0.9738 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0212 - accuracy: 0.9937 - val_loss: 0.1073 - val_accuracy: 0.9700 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0174 - accuracy: 0.9948 - val_loss: 0.0892 - val_accuracy: 0.9753 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0451 - accuracy: 0.9908 - val_loss: 0.0728 - val_accuracy: 0.9738 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.1024 - val_accuracy: 0.9693 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.0693 - val_accuracy: 0.9753 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.0702 - val_accuracy: 0.9775 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0323 - accuracy: 0.9931 - val_loss: 0.0741 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0189 - accuracy: 0.9951 - val_loss: 0.0739 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 0.0786 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 28/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0141 - accuracy: 0.9962 - val_loss: 0.0747 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 29/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0124 - accuracy: 0.9966 - val_loss: 0.0832 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 30/200\n",
      "782/792 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9951\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 0.0871 - val_accuracy: 0.9745 - lr: 0.0100\n",
      "Epoch 31/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.0817 - val_accuracy: 0.9768 - lr: 1.0000e-03\n",
      "Epoch 32/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.0831 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 33/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0101 - accuracy: 0.9975 - val_loss: 0.0828 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 34/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.0820 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 35/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0099 - accuracy: 0.9974 - val_loss: 0.0817 - val_accuracy: 0.9760 - lr: 1.0000e-03\n",
      "Epoch 36/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0098 - accuracy: 0.9975 - val_loss: 0.0818 - val_accuracy: 0.9753 - lr: 1.0000e-03\n",
      "Epoch 37/200\n",
      "785/792 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0098 - accuracy: 0.9974 - val_loss: 0.0818 - val_accuracy: 0.9760 - lr: 1.0000e-03\n",
      "Epoch 38/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0097 - accuracy: 0.9975 - val_loss: 0.0822 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 39/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.0826 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 40/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0828 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 41/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0828 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 42/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0828 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Epoch 43/200\n",
      "792/792 [==============================] - 2s 2ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0829 - val_accuracy: 0.9753 - lr: 1.0000e-04\n",
      "Score for fold 5: loss of 0.07118797302246094; accuracy of 98.17036390304565%\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = keras.models.Sequential([\n",
    "        ########## MLP\n",
    "        keras.layers.Flatten(input_shape=(X_treino.shape[1],)),\n",
    "        #keras.layers.Dense(300, activation=\"relu\"),\n",
    "        keras.layers.Dense(1000, activation=\"relu\"),\n",
    "        keras.layers.Dense(300, activation=\"relu\"),\n",
    "        keras.layers.Dense(100, activation=\"relu\"),\n",
    "\n",
    "        keras.layers.Dense(len(set(y_treino)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"sgd\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(inputs[train], targets[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.08282823115587234 - Accuracy: 97.27095365524292%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.09300064295530319 - Accuracy: 97.28554487228394%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.07573442161083221 - Accuracy: 97.51049876213074%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.07687965035438538 - Accuracy: 97.57048487663269%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.07118797302246094 - Accuracy: 98.17036390304565%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 97.56156921386719 (+- 0.3267921338453489)\n",
      "> Loss: 0.07992618381977082\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}