{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   email  target\n0      start date     hourahead timee  cardinall  hou...       0\n1      service long desk  price structure deal quote ...       0\n2      start date  cardinall    hourahead timee  card...       0\n3      start date     hourahead timee  cardinall  anc...       0\n4      cardinall deliverable revenue management marke...       0\n...                                                  ...     ...\n33340  bio  matrix scientific group   symbo   bmxg  p...       1\n33341   cardinall step away hot naked webcam girl liv...       1\n33342  need pill increase performance click  seroius ...       1\n33343  datee final nom       inlet hpl  eastrans  car...       0\n33344  ordinall time  offering male enhancement perfo...       1\n\n[33341 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>start date     hourahead timee  cardinall  hou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>service long desk  price structure deal quote ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>start date  cardinall    hourahead timee  card...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>start date     hourahead timee  cardinall  anc...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cardinall deliverable revenue management marke...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>bio  matrix scientific group   symbo   bmxg  p...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33341</th>\n      <td>cardinall step away hot naked webcam girl liv...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33342</th>\n      <td>need pill increase performance click  seroius ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33343</th>\n      <td>datee final nom       inlet hpl  eastrans  car...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33344</th>\n      <td>ordinall time  offering male enhancement perfo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"../../Database/dataBaseWithNER.csv\")\n",
    "dataset = dataset.drop(columns=[\"Unnamed: 0\"])\n",
    "dataset = dataset.dropna()\n",
    "targets = np.array(dataset[\"target\"].array)\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "emailsText = []\n",
    "for email in dataset[\"email\"]:\n",
    "    emailsText.append(email)\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33341\n"
     ]
    }
   ],
   "source": [
    "print(len(emailsText))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       aa  ability  able  absolutely  abuse  accept  acceptance  accepted  \\\n0       0        0     0           0      0       0           0         0   \n1       0        0     0           0      0       0           0         0   \n2       0        0     0           0      0       0           0         0   \n3       0        0     0           0      0       0           0         0   \n4       0        0     0           0      0       0           0         0   \n...    ..      ...   ...         ...    ...     ...         ...       ...   \n33336   0        0     0           0      0       0           1         0   \n33337   0        0     0           0      0       0           0         0   \n33338   0        0     0           0      0       0           0         0   \n33339   0        0     0           0      0       0           0         0   \n33340   0        0     0           0      0       0           0         0   \n\n       access  according  ...  xanax  xl  xp  yahoo  year  yes  yield  yo  \\\n0           0          0  ...      0   0   0      0     0    0      0   0   \n1           0          0  ...      0   0   0      0     0    0      0   0   \n2           0          0  ...      0   0   0      0     0    0      0   0   \n3           0          0  ...      0   0   0      0     0    0      0   0   \n4           1          0  ...      0   0   0      0     0    0      0   0   \n...       ...        ...  ...    ...  ..  ..    ...   ...  ...    ...  ..   \n33336       0          0  ...      0   0   0      1     0    0      0   0   \n33337       1          0  ...      0   0   0      0     0    0      0   0   \n33338       0          0  ...      0   0   0      0     0    0      0   0   \n33339       0          0  ...      0   0   0      0     0    0      0   0   \n33340       0          0  ...      0   0   0      0     0    0      0   0   \n\n       young  zone  \n0          0     0  \n1          0     0  \n2          0     0  \n3          0     0  \n4          0     0  \n...      ...   ...  \n33336      0     0  \n33337      0     0  \n33338      0     0  \n33339      0     0  \n33340      0     0  \n\n[33341 rows x 2100 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aa</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>absolutely</th>\n      <th>abuse</th>\n      <th>accept</th>\n      <th>acceptance</th>\n      <th>accepted</th>\n      <th>access</th>\n      <th>according</th>\n      <th>...</th>\n      <th>xanax</th>\n      <th>xl</th>\n      <th>xp</th>\n      <th>yahoo</th>\n      <th>year</th>\n      <th>yes</th>\n      <th>yield</th>\n      <th>yo</th>\n      <th>young</th>\n      <th>zone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>33336</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33337</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33338</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33339</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33340</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>33341 rows × 2100 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "\n",
    "bag = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "del emailsText\n",
    "del X\n",
    "\n",
    "bag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = np.array(bag)\n",
    "bag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#X_treino, X_teste, y_treino, y_teste = train_test_split(bag.values,target,test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "foldsAccuracy = []\n",
    "foldLosses = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "Iniciando treinamento da fold: 1.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 4s 3ms/step - loss: 0.3499 - accuracy: 0.8889 - val_loss: 0.1727 - val_accuracy: 0.9440 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2207 - accuracy: 0.9410 - val_loss: 0.1448 - val_accuracy: 0.9528 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9501 - val_loss: 0.1184 - val_accuracy: 0.9600 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1385 - accuracy: 0.9627 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1287 - accuracy: 0.9648 - val_loss: 0.1275 - val_accuracy: 0.9576 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0776 - accuracy: 0.9768 - val_loss: 0.0639 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0893 - accuracy: 0.9764 - val_loss: 0.0666 - val_accuracy: 0.9736 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0525 - accuracy: 0.9838 - val_loss: 0.0559 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0609 - accuracy: 0.9839 - val_loss: 0.0539 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0718 - accuracy: 0.9827 - val_loss: 0.0682 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0445 - accuracy: 0.9865 - val_loss: 0.0527 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.0443 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9882 - val_loss: 0.0532 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0288 - accuracy: 0.9915 - val_loss: 0.0550 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0246 - accuracy: 0.9930 - val_loss: 0.0423 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0233 - accuracy: 0.9939 - val_loss: 0.0417 - val_accuracy: 0.9864 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9944 - val_loss: 0.0495 - val_accuracy: 0.9824 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0448 - val_accuracy: 0.9840 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0426 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0431 - val_accuracy: 0.9856 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0254 - accuracy: 0.9946 - val_loss: 0.0464 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0193 - accuracy: 0.9954 - val_loss: 0.0476 - val_accuracy: 0.9832 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "742/743 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9961\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0150 - accuracy: 0.9961 - val_loss: 0.0538 - val_accuracy: 0.9848 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0478 - val_accuracy: 0.9872 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0128 - accuracy: 0.9968 - val_loss: 0.0466 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.0462 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.0456 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0117 - accuracy: 0.9972 - val_loss: 0.0448 - val_accuracy: 0.9880 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9971 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "743/743 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9971\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0452 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0453 - val_accuracy: 0.9864 - lr: 1.0000e-04\n",
      "Score fold 1: loss de 0.09102164953947067; accuracy de 97.5287914276123%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 2.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3240 - accuracy: 0.8913 - val_loss: 0.1648 - val_accuracy: 0.9472 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1536 - accuracy: 0.9546 - val_loss: 0.1026 - val_accuracy: 0.9584 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1171 - accuracy: 0.9664 - val_loss: 0.0876 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9588 - val_loss: 0.2787 - val_accuracy: 0.8857 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.9499 - val_loss: 0.7121 - val_accuracy: 0.7066 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.2149 - accuracy: 0.9278 - val_loss: 0.1107 - val_accuracy: 0.9560 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9687 - val_loss: 0.0864 - val_accuracy: 0.9672 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0813 - accuracy: 0.9749 - val_loss: 0.0697 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0574 - accuracy: 0.9812 - val_loss: 0.0616 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.0590 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0419 - accuracy: 0.9867 - val_loss: 0.0565 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.0586 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0774 - accuracy: 0.9845 - val_loss: 0.0612 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0520 - accuracy: 0.9876 - val_loss: 0.0577 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.0558 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0267 - accuracy: 0.9929 - val_loss: 0.0529 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0285 - accuracy: 0.9924 - val_loss: 0.0543 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0224 - accuracy: 0.9936 - val_loss: 0.0551 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.0538 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 0.0588 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.0556 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0142 - accuracy: 0.9960 - val_loss: 0.0555 - val_accuracy: 0.9808 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "723/743 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9959\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0571 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0572 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9792 - lr: 1.0000e-03\n",
      "Epoch 27/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.0574 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 28/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.0573 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0110 - accuracy: 0.9972 - val_loss: 0.0575 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "727/743 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9972\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 32/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 33/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 34/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0577 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 35/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9800 - lr: 1.0000e-04\n",
      "Epoch 36/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0108 - accuracy: 0.9972 - val_loss: 0.0576 - val_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Score fold 2: loss de 0.07085882872343063; accuracy de 97.69645929336548%\n",
      "****************************************************\n",
      "Iniciando treinamento da fold: 3.\n",
      "Epoch 1/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.3803 - accuracy: 0.8760 - val_loss: 0.2134 - val_accuracy: 0.9289 - lr: 0.0100\n",
      "Epoch 2/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9490 - val_loss: 0.1158 - val_accuracy: 0.9544 - lr: 0.0100\n",
      "Epoch 3/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9676 - val_loss: 0.0948 - val_accuracy: 0.9648 - lr: 0.0100\n",
      "Epoch 4/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1083 - accuracy: 0.9695 - val_loss: 0.0758 - val_accuracy: 0.9664 - lr: 0.0100\n",
      "Epoch 5/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0898 - accuracy: 0.9764 - val_loss: 0.0679 - val_accuracy: 0.9704 - lr: 0.0100\n",
      "Epoch 6/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0528 - accuracy: 0.9830 - val_loss: 0.0603 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 7/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.9750 - val_loss: 0.0856 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 8/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1049 - accuracy: 0.9779 - val_loss: 0.2473 - val_accuracy: 0.8753 - lr: 0.0100\n",
      "Epoch 9/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.1069 - accuracy: 0.9722 - val_loss: 0.0636 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 10/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0587 - accuracy: 0.9830 - val_loss: 0.0533 - val_accuracy: 0.9752 - lr: 0.0100\n",
      "Epoch 11/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0661 - accuracy: 0.9831 - val_loss: 0.0547 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 12/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0401 - accuracy: 0.9873 - val_loss: 0.0523 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 13/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0927 - accuracy: 0.9835 - val_loss: 0.0602 - val_accuracy: 0.9720 - lr: 0.0100\n",
      "Epoch 14/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0899 - accuracy: 0.9795 - val_loss: 0.0544 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 15/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0449 - accuracy: 0.9874 - val_loss: 0.0558 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 16/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0450 - accuracy: 0.9873 - val_loss: 0.0497 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 17/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0270 - accuracy: 0.9918 - val_loss: 0.0532 - val_accuracy: 0.9768 - lr: 0.0100\n",
      "Epoch 18/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.0515 - val_accuracy: 0.9760 - lr: 0.0100\n",
      "Epoch 19/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0463 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 20/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0253 - accuracy: 0.9931 - val_loss: 0.0453 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 21/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0457 - val_accuracy: 0.9784 - lr: 0.0100\n",
      "Epoch 22/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0203 - accuracy: 0.9947 - val_loss: 0.0481 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 23/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0170 - accuracy: 0.9949 - val_loss: 0.0502 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 24/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 0.0512 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 25/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0139 - accuracy: 0.9958 - val_loss: 0.0589 - val_accuracy: 0.9776 - lr: 0.0100\n",
      "Epoch 26/200\n",
      "743/743 [==============================] - 2s 2ms/step - loss: 0.0404 - accuracy: 0.9922 - val_loss: 0.0557 - val_accuracy: 0.9800 - lr: 0.0100\n",
      "Epoch 27/200\n",
      "  1/743 [..............................] - ETA: 1s - loss: 0.0097 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "foldCount = 1\n",
    "for train, test in kfold.split(bag, targets):\n",
    "    model = keras.models.Sequential([\n",
    "        ########## MLP\n",
    "        keras.layers.Flatten(input_shape=(bag.shape[1],)),\n",
    "        #keras.layers.Dense(300, activation=\"relu\"),\n",
    "        keras.layers.Dense(1000, activation=\"relu\"),\n",
    "        keras.layers.Dense(300, activation=\"relu\"),\n",
    "        keras.layers.Dense(100, activation=\"relu\"),\n",
    "\n",
    "        keras.layers.Dense(len(set(targets)), activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "    print('****************************************************')\n",
    "    print(f'Iniciando treinamento da fold: {foldCount}.')\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4,mode='min'), tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=0, restore_best_weights=True)]\n",
    "\n",
    "    history = model.fit(bag[train], targets[train], epochs=200, callbacks=callbacks, validation_split=0.05)\n",
    "\n",
    "    scores = model.evaluate(bag[test], targets[test], verbose=0)\n",
    "    print(f'Score fold {foldCount}: {model.metrics_names[0]} de {scores[0]}; {model.metrics_names[1]} de {scores[1]*100}%')\n",
    "\n",
    "    foldsAccuracy.append(scores[1] * 100)\n",
    "    foldLosses.append(scores[0])\n",
    "\n",
    "    foldCount = foldCount + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('****************************************************')\n",
    "print('Score de cada fold:')\n",
    "for i in range(0, len(foldsAccuracy)):\n",
    "    print('****************************************************')\n",
    "    print(f'--> Fold {i+1}: Loss: {foldLosses[i]} ; Accuracy: {foldsAccuracy[i]}%')\n",
    "\n",
    "print('****************************************************')\n",
    "print('Média de accuracy das folds:')\n",
    "print(f'--> Accuracy: {np.mean(foldsAccuracy)} (+- {np.std(foldsAccuracy)})')\n",
    "print(f'--> Loss: {np.mean(foldLosses)}')\n",
    "print('****************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}