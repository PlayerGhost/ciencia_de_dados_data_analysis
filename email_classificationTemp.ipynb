{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import math\n",
    "#from langdetect import detect,detect_langs\n",
    "import validators\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "#import en_core_web_sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer,word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CARREGANDO BASE DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''teste = pd.read_csv(\"spam_ham_dataset.csv\").sample(frac=1)\n",
    "teste2 =pd.read_csv(\"emails.csv\").sample(frac=1)\n",
    "targetTest2 = teste2[\"spam\"]\n",
    "teste = teste.drop(columns=[\"Unnamed: 0\",\"label_num\"])\n",
    "teste = teste.replace({\"ham\":0,\"spam\":1})\n",
    "targetTest = teste[\"label\"]'''\n",
    "\n",
    "database = pd.read_csv(\"Database/enron_spam_data.csv\")\n",
    "database = database.drop(columns=[\"Message ID\",\"Date\"])\n",
    "database = database.replace({\"ham\":0,\"spam\":1})\n",
    "database = database.drop(columns=[\"Subject\"])\n",
    "database = database.dropna()\n",
    "database = database.sample(frac=1)\n",
    "target = database[\"Spam/Ham\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    ponctuation = list(punctuation)\n",
    "\n",
    "    for i in ponctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "stopWords = set(stopwords.words('english')  + list(punctuation) + list(STOPWORDS))\n",
    "stopWords.add(\"subject\")\n",
    "stem = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "regex = re.compile(r'[\\w\\.-]+@[\\w\\.-]+(\\.[\\w]+)+')\n",
    "\n",
    "def wordsPreProcessing(email):\n",
    "    if email is None:\n",
    "        return 'empty'\n",
    "\n",
    "    newText = \"\"\n",
    "\n",
    "    e = email.split()\n",
    "\n",
    "    for i in range(0,len(e)):\n",
    "        if validators.url(e[i]):\n",
    "            e[i] = \"URLLL\"\n",
    "\n",
    "        if re.fullmatch(regex,e[i]):\n",
    "            e[i] = \"EMAILLL\"\n",
    "\n",
    "    for text in word_tokenize(email.lower()):\n",
    "        text = removePunctuation(text)\n",
    "        if text not in stopWords and not text.isdigit():\n",
    "            newText += lemmatizer.lemmatize(text) + \" \"\n",
    "\n",
    "    return newText\n",
    "\n",
    "def NERProcessing(emails):\n",
    "    emailsPreProcessedLocal = []\n",
    "\n",
    "    spacy.require_gpu()\n",
    "    NER = spacy.load(\"en_core_web_sm\")\n",
    "    NER.disable_pipe(\"parser\")\n",
    "    NER.add_pipe(\"sentencizer\")\n",
    "    NER.add_pipe(\"merge_entities\")\n",
    "\n",
    "    for doc in NER.pipe(emails, n_process=2):\n",
    "        emailsPreProcessedAux = []\n",
    "\n",
    "        for t in doc:\n",
    "            if not t.ent_type_:\n",
    "                emailsPreProcessedAux.append(t.text)\n",
    "            else:\n",
    "                emailsPreProcessedAux.append((t.ent_type_ + t.ent_type_[-1]).upper())\n",
    "\n",
    "        emailsPreProcessedAux = \" \".join(emailsPreProcessedAux)\n",
    "        emailsPreProcessedLocal.append(wordsPreProcessing(emailsPreProcessedAux))\n",
    "\n",
    "    return emailsPreProcessedLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWord(emails):\n",
    "    wordCount = {}\n",
    "\n",
    "    for email in emails:\n",
    "        for i in email.split():\n",
    "            if i not in wordCount.keys():\n",
    "                wordCount[i] = 1\n",
    "            else:\n",
    "                wordCount[i] += 1\n",
    "\n",
    "    return wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # emailsTest = []\n",
    "# #\n",
    "# # for email in np.array(database[\"Message\"]):\n",
    "# #     emailsTest.append(NERProcessing(email))\n",
    "# #\n",
    "# # NERDataframe = pd.DataFrame(emailsTest, columns=[\"email\"])\n",
    "# # NERDataframe.to_csv('Database/dataBaseWithNER.csv')\n",
    "# # NERDataframe\n",
    "# a = wordsPreProcessing(database[\"Message\"].array[0])\n",
    "# a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\spacy\\language.py:1568: UserWarning: [W114] Using multiprocessing with GPU models is not recommended and may lead to errors.\n",
      "  warnings.warn(Warnings.W114)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m textos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m ini \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m----> 4\u001b[0m emailsPreProcessed \u001b[38;5;241m=\u001b[39m \u001b[43mNERProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#print(emailsPreProcessed)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#print()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m ini)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mNERProcessing\u001b[1;34m(emails)\u001b[0m\n\u001b[0;32m     54\u001b[0m             emailsPreProcessedAux\u001b[38;5;241m.\u001b[39mappend((t\u001b[38;5;241m.\u001b[39ment_type_ \u001b[38;5;241m+\u001b[39m t\u001b[38;5;241m.\u001b[39ment_type_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mupper())\n\u001b[0;32m     56\u001b[0m     emailsPreProcessedAux \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(emailsPreProcessedAux)\n\u001b[1;32m---> 57\u001b[0m     emailsPreProcessedLocal\u001b[38;5;241m.\u001b[39mappend(\u001b[43mwordsPreProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43memailsPreProcessedAux\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m emailsPreProcessedLocal\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mwordsPreProcessing\u001b[1;34m(email)\u001b[0m\n\u001b[0;32m     22\u001b[0m e \u001b[38;5;241m=\u001b[39m email\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(e)):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mvalidators\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     26\u001b[0m         e[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURLLL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfullmatch(regex,e[i]):\n",
      "File \u001b[1;32mE:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mE:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\validators\\utils.py:86\u001b[0m, in \u001b[0;36mvalidator.<locals>.wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m value \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ValidationFailure(\n\u001b[1;32m---> 86\u001b[0m         func, \u001b[43mfunc_args_as_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mE:\\DevPack\\anaconda3\\envs\\data_science\\lib\\site-packages\\validators\\utils.py:48\u001b[0m, in \u001b[0;36mfunc_args_as_dict\u001b[1;34m(func, args, kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     _getargspec \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetfullargspec\n\u001b[0;32m     45\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m     46\u001b[0m     OrderedDict\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m     47\u001b[0m         itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m---> 48\u001b[0m             \u001b[43m_getargspec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     49\u001b[0m             kwargs\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m     50\u001b[0m         )\n\u001b[0;32m     51\u001b[0m     )\n\u001b[0;32m     52\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mlist\u001b[39m(six\u001b[38;5;241m.\u001b[39mmoves\u001b[38;5;241m.\u001b[39mzip(arg_names, args)) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m     56\u001b[0m )\n",
      "File \u001b[1;32mE:\\DevPack\\anaconda3\\envs\\data_science\\lib\\inspect.py:1196\u001b[0m, in \u001b[0;36mgetfullargspec\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(name)\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m-> 1196\u001b[0m         defaults \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m,)\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind \u001b[38;5;129;01mis\u001b[39;00m _VAR_POSITIONAL:\n\u001b[0;32m   1198\u001b[0m     varargs \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[1;32mE:\\DevPack\\anaconda3\\envs\\data_science\\lib\\inspect.py:2554\u001b[0m, in \u001b[0;36mParameter.default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2552\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "textos = []\n",
    "ini = datetime.now()\n",
    "\n",
    "emailsPreProcessed = NERProcessing(database[\"Message\"])\n",
    "\n",
    "#print(emailsPreProcessed)\n",
    "#print()\n",
    "print(datetime.now() - ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NERDataframe = pd.DataFrame(emailsPreProcessed, columns=[\"email\"])\n",
    "NERDataframe[\"target\"] = target.values\n",
    "\n",
    "NERDataframe.to_csv('Database/dataBaseWithNER.csv')\n",
    "NERDataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "database = pd.read_csv(\"Database/dataBaseWithNER.csv\")\n",
    "\n",
    "database = database.drop(columns=[\"Unnamed: 0\"])\n",
    "database = database.dropna()\n",
    "target = database[\"target\"].array\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#wordCount = bagOfWord(emailsText)\n",
    "\n",
    "#wordFrequency = heapq.nlargest(100, wordCount, wordCount.get)\n",
    "\n",
    "#print(wordFrequency)\n",
    "emailsText = []\n",
    "for email in database[\"email\"]:\n",
    "  emailsText.append(email)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(emailsText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer(analyzer=\"word\",max_features=2100)\n",
    "#tfidfVectorizer = TfidfVectorizer(analyzer=\"word\")\n",
    "\n",
    "tfidfTransform = tfidfVectorizer.fit_transform(emailsText)\n",
    "\n",
    "print(tfidfTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidfLabels = tfidfVectorizer.get_feature_names()\n",
    "tfidfLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfTfidfArray = pd.DataFrame(data=tfidfTransform.toarray(), columns=tfidfLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dfTfidfArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dfTfidfArray.insert(len(dfTfidfArray.columns), \"Target\", target, True)\n",
    "dfTfidfArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dfTfidfArray.to_csv(\"dataset.csv\", sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, random_state=0)\n",
    "#model = PCA(n_components=50, svd_solver='full')\n",
    "array_red = model.fit_transform(dfTfidfArray)\n",
    "\n",
    "df_tsne = pd.DataFrame(array_red)\n",
    "\n",
    "target = dfTfidfArray[\"Target\"].array\n",
    "\n",
    "df_tsne['Target'] = target\n",
    "df_tsne_c1 = df_tsne[df_tsne['Target'] == 0]\n",
    "\n",
    "df_tsne_c2 = df_tsne[df_tsne['Target'] == 1]\n",
    "\n",
    "plt.scatter(df_tsne_c1[0].array,df_tsne_c1[1].array,marker='o',color='blue')\n",
    "\n",
    "plt.scatter(df_tsne_c2[0].array,df_tsne_c2[1].array,marker='o',color='red')\n",
    "\n",
    "plt.title('Dados')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_features = dfTfidfArray.drop(columns=['Target'])\n",
    "#df_tsneTarget = df_tsne[\"Target\"].array\n",
    "#df_tsneFeatures = df_tsne.drop(columns=['Target'])\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getModel():\n",
    "      return LogisticRegression(max_iter=400)\n",
    "#     return DecisionTreeClassifier()\n",
    "#     return RandomForestClassifier()\n",
    "#     return LinearSVC()\n",
    "#     return MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(df_features.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(getModel(),df_features.values,target,cv=10)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicoes = cross_val_predict(getModel(), df_features.values, target, cv=10)\n",
    "\n",
    "cm = confusion_matrix(target,predicoes,labels=[0, 1])\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=[0, 1])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(max_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "\n",
    "bag = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "bag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(bag.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "#score = modelo.score([\"alo\"],[1])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicoes = cross_val_predict(getModel(), bag.values, target, cv=10)\n",
    "\n",
    "cm = confusion_matrix(target,predicoes,labels=[0, 1])\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=[0, 1])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "hashing = pd.DataFrame(X.toarray())\n",
    "hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(hashing.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "#score = modelo.score([\"alo\"],[1])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicoes = cross_val_predict(getModel(), hashing.values, target, cv=10)\n",
    "\n",
    "cm = confusion_matrix(target,predicoes,labels=[0, 1])\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=[0, 1])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest2 = []\n",
    "\n",
    "for email in t[\"text\"]:\n",
    "    emailsTest2.append(NERProcessing(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest3 = []\n",
    "\n",
    "for email in teste[\"text\"]:\n",
    "    emailsTest3.append(wordsPreProcessing(email))\n",
    "    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new_counts = tfidfVectorizer.transform(emailsTest2)\n",
    "\n",
    "X_new_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "hashing = pd.DataFrame(X.toarray())\n",
    "hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(hashing.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "#score = modelo.score([\"alo\"],[1])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicoes = cross_val_predict(getModel(), hashing.values, target, cv=10)\n",
    "\n",
    "cm = confusion_matrix(target,predicoes,labels=[0, 1])\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=[0, 1])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest2 = []\n",
    "\n",
    "for email in t[\"text\"]:\n",
    "    emailsTest2.append(NERProcessing(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest3 = []\n",
    "\n",
    "for email in teste[\"text\"]:\n",
    "    emailsTest3.append(wordsPreProcessing(email))\n",
    "    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new_counts = tfidfVectorizer.transform(emailsTest2)\n",
    "\n",
    "X_new_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "hashing = pd.DataFrame(X.toarray())\n",
    "hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(hashing.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "#score = modelo.score([\"alo\"],[1])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicoes = cross_val_predict(getModel(), hashing.values, target, cv=10)\n",
    "\n",
    "cm = confusion_matrix(target,predicoes,labels=[0, 1])\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=[0, 1])\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest2 = []\n",
    "\n",
    "for email in t[\"text\"]:\n",
    "    emailsTest2.append(NERProcessing(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest3 = []\n",
    "\n",
    "for email in teste[\"text\"]:\n",
    "    emailsTest3.append(wordsPreProcessing(email))\n",
    "    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new_counts = tfidfVectorizer.transform(emailsTest2)\n",
    "\n",
    "X_new_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=2100)\n",
    "X = vectorizer.fit_transform(emailsText)\n",
    "hashing = pd.DataFrame(X.toarray())\n",
    "hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_treino, X_teste, y_treino, y_teste = train_test_split(hashing.values,target,test_size=0.2)\n",
    "modelo = getModel().fit(X_treino,y_treino)\n",
    "score = modelo.score(X_teste,y_teste)\n",
    "#score = modelo.score([\"alo\"],[1])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest2 = []\n",
    "\n",
    "for email in t[\"text\"]:\n",
    "    emailsTest2.append(NERProcessing(email))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emailsTest3 = []\n",
    "\n",
    "for email in teste[\"text\"]:\n",
    "    emailsTest3.append(wordsPreProcessing(email))\n",
    "    \n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new_counts = tfidfVectorizer.transform(emailsTest2)\n",
    "\n",
    "X_new_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#APAGAR DEPOIS\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_train_counts = count_vect.fit_transform(emailsTest)\n",
    "# X_train_counts.shape\n",
    "#\n",
    "# tf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tf_transformer.fit_transform(X_train_counts)\n",
    "# X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ##APAGAR DEPOIS\n",
    "# clf = getModel().fit(X_train_tfidf,target)\n",
    "#\n",
    "# X_new_counts = count_vect.transform(emailsTest2)\n",
    "#\n",
    "# X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "#\n",
    "# predict = clf.predict(X_new_tfidf)\n",
    "# print(len(emailsTest2))\n",
    "# print(predict)\n",
    "# print(targetTest2)\n",
    "# print(teste2)\n",
    "# print(clf.score(X_new_tfidf,targetTest2))\n",
    "#\n",
    "# #predicoesTest = cross_val_predict(modelo2, dfTfidfArrayFeaturesTest.values, dfTfidfArrayTargetTest, cv=10)\n",
    "#\n",
    "# cmTest = confusion_matrix(targetTest2,predict,labels=[0, 1])\n",
    "#\n",
    "# cm_dfTest = pd.DataFrame(cmTest, columns=[0, 1])\n",
    "#\n",
    "# cm_dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dfTfidfArrayTest = pd.DataFrame(data=tfidfTransformTest.toarray(), columns=tfidfVectorizerTest.get_feature_names_out())\n",
    "# dfTfidfArrayTest.insert(len(dfTfidfArrayTest.columns), \"Target\", target.array, True)\n",
    "# dfTfidfArrayTest = dfTfidfArrayTest.sample(frac=1)\n",
    "# dfTfidfArrayTest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# modelTest = TSNE(n_components=2, random_state=0)\n",
    "# #modelTest = PCA(n_components=50)\n",
    "# array_redTest = modelTest.fit_transform(dfTfidfArrayTest)\n",
    "#\n",
    "# df_tsneTest = pd.DataFrame(array_redTest)\n",
    "# df_tsneTest['Target'] = target.array\n",
    "#\n",
    "# df_tsne_c1Test = df_tsneTest[df_tsneTest['Target'] == 0]\n",
    "#\n",
    "# df_tsne_c2Test  = df_tsneTest[df_tsneTest['Target'] == 1]\n",
    "#\n",
    "# plt.scatter(df_tsne_c1Test[0].array,df_tsne_c1Test[1].array,marker='o',color='blue')\n",
    "#\n",
    "# plt.scatter(df_tsne_c2Test[0].array,df_tsne_c2Test[1].array,marker='o',color='red')\n",
    "#\n",
    "# plt.title('Dados')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "#\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dfTfidfArrayTarget = dfTfidfArray[\"Target\"].array\n",
    "# dfTfidfArrayFeatures = dfTfidfArray.drop(columns=['Target'])\n",
    "#\n",
    "# print(dfTfidfArrayFeatures)\n",
    "#\n",
    "# dfTfidfArrayTargetTest = dfTfidfArrayTest[\"Target\"].array\n",
    "# dfTfidfArrayFeaturesTest = dfTfidfArrayTest.drop(columns=['Target'])\n",
    "#\n",
    "#\n",
    "#\n",
    "# print(dfTfidfArrayFeaturesTest)\n",
    "# dfTfidfArrayTargetTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dfTfidfArrayTargetTest2 = dfTfidfArrayTest2[\"Target\"].array\n",
    "# dfTfidfArrayFeaturesTest2 = dfTfidfArrayTest2.drop(columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# #from sklearn.svm import LinearSVC\n",
    "#\n",
    "# X_treino, X_teste, y_treino, y_teste = train_test_split(dfTfidfArrayFeaturesTest2.values,dfTfidfArrayTargetTest2,test_size=0.2)\n",
    "# X_treinoTest, X_testeTest, y_treinoTest, y_testeTest = train_test_split(dfTfidfArrayFeaturesTest.values,dfTfidfArrayTargetTest,test_size=0.01)\n",
    "# modelo = getModel().fit(X_treino,y_treino)\n",
    "# modelo2 = getModel().fit(X_treinoTest,y_treinoTest)\n",
    "# predict = modelo.predict(X_teste)\n",
    "# #score = modelo.score(X_treinoTest,y_treinoTest)\n",
    "# #score2 = modelo2.score(X_treino,y_treino)\n",
    "#\n",
    "# cmTest = confusion_matrix(y_teste,predict)\n",
    "# cm_dfTest = pd.DataFrame(cmTest, columns=[0, 1])\n",
    "#\n",
    "# cm_dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#predicoesTest = cross_val_predict(modelo2, dfTfidfArrayFeaturesTest.values, dfTfidfArrayTargetTest, cv=10)\n",
    "\n",
    "#cmTest = confusion_matrix(dfTfidfArrayTargetTest,predicoesTest,labels=[0, 1])\n",
    "\n",
    "#cm_dfTest = pd.DataFrame(cmTest, columns=[0, 1])\n",
    "\n",
    "#cm_dfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
